{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意！！！！！ 需要更改训练函数的维度以及最后两个向量的相似度计算！！！！与AI研习社中的计算方法相同#\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD,EOS='<pad>','<eos>'\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理数据对\n",
    "def processline(x,y):\n",
    "    # 去除各种杂乱的标点符号\n",
    "    x=re.sub('[.(),->]',' ',x)\n",
    "    #使用strip去除y尾部的换行符\n",
    "    y=re.sub(r'[()]',' ',y.strip())\n",
    "    #  把#部分与前面类名用空格分开,其中line为字符串\n",
    "    x=re.sub('[#]',' #',x)\n",
    "    #使用line1存储处理过后的x字符串，line2存储处理过后的y字符串\n",
    "    line1=''\n",
    "    line2=''\n",
    "    for i in x.split(' '):\n",
    "        if i=='':\n",
    "            continue\n",
    "        if not re.match('#.*',i):\n",
    "            line1+=i+' '\n",
    "        else:      \n",
    "            #下面语句是将方法名按照大写字母分割开,对于y为空的语句将#后面切割之后的方法名替代\n",
    "            if y=='':\n",
    "                y=re.sub(\"[A-Z]\",lambda x:\" \"+x.group(0),i[1:])\n",
    "            line1+=(re.sub(\"[A-Z]\",lambda x:\" \"+x.group(0),i[1:]))+' '\n",
    "    for j in y.split(' '):\n",
    "        if j=='':\n",
    "            continue\n",
    "        else:\n",
    "            line2+=j+' '   \n",
    "    # 返回处理之后的x与y字符串\n",
    "    return line1.lower().strip(),line2.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列\n",
    "# 长度变为max_seq_len，然后将序列保存在all_seqs中\n",
    "def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):\n",
    "    all_tokens.extend(seq_tokens)\n",
    "    seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)\n",
    "    all_seqs.append(seq_tokens)\n",
    "\n",
    "# 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造Tensor\n",
    "def build_data(all_tokens, all_seqs):\n",
    "    vocab = Vocab.Vocab(collections.Counter(all_tokens),\n",
    "                        specials=[PAD, EOS])\n",
    "    indices = [[vocab.stoi[w] for w in seq] for seq in all_seqs]\n",
    "    return vocab, torch.tensor(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 开始读取数据，构建词典，以及数据集\n",
    "def read_data(path,query_max_length,api_max_length):\n",
    "    in_tokens,out_tokens,in_seqs,out_seqs=[],[],[],[]\n",
    "    with io.open(path) as f:\n",
    "        lines=f.readlines()\n",
    "    for line in lines:\n",
    "        #将每一行语句按照'：'将api与对应的query查询分开\n",
    "        out_seq,in_seq=line.split(':::')\n",
    "        #使用processline处理输入输出数据的各种标点，输出干净字符串\n",
    "        out_seq,in_seq=processline(out_seq,in_seq)\n",
    "        \n",
    "        in_seq_tokens,out_seq_tokens=in_seq.split(' '),out_seq.split(' ')\n",
    "       #针对描述语句过长的进行截断，下面减一操作是因为要对句末添加EOS\n",
    "        if len(in_seq_tokens)>query_max_length-1:\n",
    "            in_seq_tokens=in_seq_tokens[:query_max_length-1]\n",
    "        #api序列名数据比较规整可以使用最大长度为api_max_length，对于短于最大值的api序列进行padding\n",
    "#       if len(out_seq_tokens)>api_max_length-1:\n",
    "#          out_seq_tokens=out_seq_tokens[:api_max_length]\n",
    "        process_one_seq(in_seq_tokens,in_tokens,in_seqs,query_max_length)\n",
    "        process_one_seq(out_seq_tokens,out_tokens,out_seqs,api_max_length)\n",
    "    in_vocab,in_data=build_data(in_tokens,in_seqs)\n",
    "    out_vocab,out_data=build_data(out_tokens,out_seqs)\n",
    "\n",
    "    return in_vocab,out_vocab,Data.TensorDataset(in_data,out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语义相似度GRU模型\n",
    "class GRU(nn.Module):\n",
    "    #其中input_size是指词表的大小\n",
    "    def __init__(self,input_size,hidden_size,out_size):\n",
    "        super(GRU,self).__init__()\n",
    "        \n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding=nn.Embedding(input_size,hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, out_size)\n",
    "        self.state = None\n",
    "    def forward(self,input, state):\n",
    "        # 输入形状是(批量大小, 时间步数)。将输出互换样本维和时间步维\n",
    "        #理解此处的0与1维度互换，因为是GRU序列，按照文本序列一个词一个词的输入进循环网络\n",
    "        embedded=self.embedding(input.long()).permute(1,0,2)# (seq_len, batch, input_size)\n",
    "        output=embedded\n",
    "        output,state=self.gru(output,state)\n",
    "        #移除时间步维，输出形状为(批量大小, 输出词典大小)\n",
    "        #疑问：state是否是最后linear层的输入,输出的尺寸应该为（GRU层数，batch_size，num_hidden）\n",
    "        return self.out(state).squeeze(dim=0)\n",
    "\n",
    "    #def initRNN(self):\n",
    "     #   return None\n",
    "# 例子：创建一个批量大小为4、时间步数为7的小批量序列输入。设门控循环单元的隐藏层个数为2，隐藏单元个数为16。\n",
    "# encoder = Encoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "# output, state = encoder(torch.zeros((4, 7)), encoder.begin_state())\n",
    "# output.shape, state.shape # GRU的state是h, 而LSTM的是一个元组(h, c)\n",
    "\n",
    "# 其中输出形状output为(时间步数, 批量大小, 隐藏单元个数)  torch.Size([7, 4, 16])\n",
    "# 返回的状态为state为(隐藏层个数, 批量大小, 隐藏单元个数) torch.Size([2, 4, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#小批量计算损失\n",
    "def batch_loss(rnn_q,rnn_api,query,api,tag,loss, tmperature = 0.3):\n",
    "    batch_size=query.shape[0]\n",
    "#   初始化GRU的隐藏层状态\n",
    "    q_state=None\n",
    "    api_state=None\n",
    "    q_output=rnn_q(query,q_state)\n",
    "    api_output=rnn_api(api,api_state)\n",
    "    #此时输出的维度猜测应该是（batch_size,hidden_size）\n",
    "    #下方为AI研习社语义任务之后修改的向量维度\n",
    "    #\n",
    "    # print(q_output.shape)\n",
    "    # print(api_output.shape)\n",
    "    q_output=q_output/torch.norm(q_output,dim=1).unsqueeze(1)\n",
    "    api_output=api_output/torch.norm(api_output,dim=1).unsqueeze(1)\n",
    "    \n",
    "    pre_score=torch.mm(q_output,api_output.permute(1,0))\n",
    "    #此处进行sigmoid方程与结果的概率进行（0-1）匹配。可能存在一部分问题，待确定\n",
    "    score=torch.sigmoid(torch.flatten(pre_score))\n",
    "  #！！！！！这个l的初始化可能有问题！！！！\n",
    "    #tag应该是一个1维向量（由对角矩阵拉直）\n",
    "    l=loss(score / tmperature,tag/tmperature).mean()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练过程\n",
    "def train(rnn_q,rnn_api,dataset,lr,batch_size,num_epochs):\n",
    "    query_optimizer=torch.optim.Adam(rnn_q.parameters(),lr=lr)\n",
    "    api_optimizier=torch.optim.Adam(rnn_api.parameters(),lr=lr)\n",
    "    #将损失函数由交叉熵损失改为MSE损失\n",
    "    loss=nn.MSELoss(reduction='none')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        data_iter=Data.DataLoader(dataset,batch_size,shuffle=True)\n",
    "        ##print(\"epoch为：{}, iter: {}\".format(epoch, len(data_iter)))\n",
    "        l_sum=0.0\n",
    "        for iter, (query,api) in enumerate(data_iter):\n",
    "            query_optimizer.zero_grad()\n",
    "            api_optimizier.zero_grad()\n",
    "            res_tag=torch.flatten(torch.eye(query.shape[0],query.shape[0]))\n",
    "            l=batch_loss(rnn_q,rnn_api,query,api,res_tag,loss)\n",
    "            l.backward()\n",
    "            query_optimizer.step()\n",
    "            api_optimizier.step()\n",
    "            l_sum+=l.item()\n",
    "            if (iter + 1) % 10 == 0:\n",
    "                print(\"iter[{}/{}], loss {:.4f}\".format((iter+1)*batch_size, len(dataset), l_sum / (iter+1)))\n",
    "        print(\"epoch[{}/{}], loss {:.4f}\".format(epoch, num_epochs, l_sum/len(data_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.vocab.Vocab object at 0x0000018E9D5B2408>\n",
      "epoch为：\n",
      "0\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-dc13095affd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mrnn_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mrnn_api\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mres_tag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-c9991c490a79>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(rnn_q, rnn_api, dataset, lr, batch_size, num_epochs, tag)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mapi_optimizier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_q\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrnn_api\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mquery_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mapi_optimizier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "#设置相关超参数\n",
    "hidden_size,out_size = 128, 64\n",
    "lr, batch_size, num_epochs =0.01, 8, 20\n",
    "\n",
    "#注意此时 res_tag的形状即为（batch_size,batch_size）\n",
    "\n",
    "\n",
    "#假设此时的batch_size为2，应该是一个对角矩阵（对角线均为1.0）\n",
    "query_max_length=20\n",
    "#需要提前计算api数据集中api序列的最大长度\n",
    "api_max_length=10\n",
    "path=\"./test.txt\"\n",
    "in_vocab,out_vocab,dataset=read_data(path,query_max_length,api_max_length)\n",
    "print(in_vocab)\n",
    "rnn_q = GRU(len(in_vocab),hidden_size,out_size)\n",
    "rnn_api = GRU(len(in_vocab),hidden_size,out_size)\n",
    "train(rnn_q, rnn_api, dataset, lr, batch_size, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#收集训练后的API特征向量\n",
    "#注意此时的DataLoader生成的序列应该只包含分类为1的query与api\n",
    "data_iter=Dara.DataLoader(dataset,batch_size,shuffle=True)\n",
    "def collection(rnn_api,data_iter):\n",
    "    api_output=torch.tensor([0.0])\n",
    "    api=[]\n",
    "    for query,api in data_iter:\n",
    "    #初始化rnn_api的初始隐层状态\n",
    "        api=api.append(api)\n",
    "        api_state=rnn_api.initRNN()\n",
    "        output=rnn_api(api,api_state)\n",
    "        api_output=torch.cat((api_output.expand(outpur.shape),output))\n",
    "    return api_output,api\n",
    "\n",
    "#   注 意！！！\n",
    "#   需要把输出的api_output初始expand函数产生的全为零的部分清除,之后才可以保存到csv文件中\n",
    "        \n",
    "# 对collection函数进行调用，得到api_outpur(特征向量)，api（api的字符串序列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下内容是对不熟悉函数的测试，不属于模型的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([10, 11]), tensor([20, 21]), tensor(1))\n",
      "(tensor([12, 13]), tensor([22, 23]), tensor(2))\n"
     ]
    }
   ],
   "source": [
    "# test ：测试Data.TensorDataset类功能\n",
    "# res=torch.tensor([1,2])\n",
    "in1=torch.tensor([[10,11],[12,13]])\n",
    "out=torch.tensor([[20,21],[22,23]])\n",
    "hh=Data.TensorDataset(in1,out,res)\n",
    "for h in hh:\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "torch.Size([2])\n",
      "[[0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec</th>\n",
       "      <th>apiseq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          vec apiseq\n",
       "0  [0.11999999731779099, 0.20000000298023224]   api1\n",
       "1  [0.11999999731779099, 0.20000000298023224]   api2\n",
       "2  [0.11999999731779099, 0.20000000298023224]   api3\n",
       "3  [0.11999999731779099, 0.20000000298023224]   api4\n",
       "4  [0.11999999731779099, 0.20000000298023224]   api5\n",
       "5  [0.11999999731779099, 0.20000000298023224]   api6\n",
       "6  [0.11999999731779099, 0.20000000298023224]   api7\n",
       "7  [0.11999999731779099, 0.20000000298023224]   api8"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test: 测试tensor输出为csv格式的文件\n",
    "\n",
    "import csv\n",
    "\n",
    "d=torch.tensor([0.0])\n",
    "# for i in range(3):\n",
    "out=torch.tensor([[0.12,0.2],[0.12,0.2],[0.12,0.2],[0.12,0.2]])\n",
    "print(out.shape)\n",
    "print(out[0].shape)\n",
    "out=torch.cat((out.expand(out.shape),out))\n",
    "out=out.data.numpy().tolist()\n",
    "print(out)\n",
    "apiseq=['api1','api2','api3','api4','api5','api6','api7','api8']\n",
    "data=pd.DataFrame(columns=['vec','apiseq'])\n",
    "# res=pd.DataFrame(data=out)\n",
    "data['vec'],data['apiseq']=out,apiseq\n",
    "\n",
    "# res.to_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 1., 0., 0.])\n",
      "tensor([0.7311, 0.5000, 0.5000, 0.7311, 0.5000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "#单位矩阵\n",
    "import torch\n",
    "data=torch.flatten(torch.eye(3,2))\n",
    "print(data)\n",
    "data=torch.sigmoid(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "编辑元数据",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda29ca87a343a1497fa9db5a03b8930b5d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
