{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意！！！！！ 需要更改训练函数的维度以及最后两个向量的相似度计算！！！！与AI研习社中的计算方法相同#\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD,EOS,UNK='<pad>','<eos>','<unk>'\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "api_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理数据对\n",
    "def processline(x,y):\n",
    "    # 去除各种杂乱的标点符号\n",
    "    x=re.sub('[.(),->]',' ',x)\n",
    "    #使用strip去除y尾部的换行符\n",
    "    y=re.sub(r'[()]',' ',y.strip())\n",
    "    #  把#部分与前面类名用空格分开,其中line为字符串\n",
    "    x=re.sub('[#]',' #',x)\n",
    "    #使用line1存储处理过后的x字符串，line2存储处理过后的y字符串\n",
    "    line1=''\n",
    "    line2=''\n",
    "    for i in x.split(' '):\n",
    "        if i=='':\n",
    "            continue\n",
    "        if not re.match('#.*',i):\n",
    "            line1+=i+' '\n",
    "        else:      \n",
    "            #下面语句是将方法名按照大写字母分割开,对于y为空的语句将#后面切割之后的方法名替代\n",
    "#             if y=='':\n",
    "#                 y=re.sub(\"[A-Z]\",lambda x:\" \"+x.group(0),i[1:])\n",
    "            line1+=(re.sub(\"[A-Z]\",lambda x:\" \"+x.group(0),i[1:]))+' '\n",
    "    for j in y.split(' '):\n",
    "        if j=='':\n",
    "            continue\n",
    "        else:\n",
    "            line2+=j+' '   \n",
    "    # 返回处理之后的x与y字符串\n",
    "    return line1.lower().strip(),line2.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列\n",
    "# 长度变为max_seq_len，然后将序列保存在all_seqs中\n",
    "def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):\n",
    "    all_tokens.extend(seq_tokens)\n",
    "    seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)\n",
    "    all_seqs.append(seq_tokens)\n",
    "\n",
    "# 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造Tensor\n",
    "def build_data(all_tokens, all_seqs):\n",
    "    vocab = Vocab.Vocab(collections.Counter(all_tokens),\n",
    "                        specials=[PAD, EOS,UNK])\n",
    "    indices = [[vocab.stoi[w] for w in seq] for seq in all_seqs]\n",
    "    return vocab, torch.tensor(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 开始读取数据，构建词典，以及数据集\n",
    "def read_data(path,query_max_length,api_max_length):\n",
    "#     使用raw_apis列表装所有的原生api\n",
    "    in_tokens,out_tokens,in_seqs,out_seqs,raw_apis=[],[],[],[],[]\n",
    "    with io.open(path) as f:\n",
    "        lines=f.readlines()\n",
    "    for line in lines:\n",
    "        #将每一行语句按照'：'将api与对应的query查询分开\n",
    "        out_seq,in_seq=line.split(':::')\n",
    "        raw_apis.append(out_seq)\n",
    "        #使用processline处理输入输出数据的各种标点，输出干净字符串\n",
    "        out_seq,in_seq=processline(out_seq,in_seq.strip())\n",
    "        \n",
    "        in_seq_tokens,out_seq_tokens=in_seq.split(' '),out_seq.split(' ')\n",
    "       #针对描述语句过长的进行截断，下面减一操作是因为要对句末添加EOS\n",
    "        if len(in_seq_tokens)>query_max_length-1:\n",
    "            in_seq_tokens=in_seq_tokens[:query_max_length-1]\n",
    "        #api序列名数据比较规整可以使用最大长度为api_max_length，对于短于最大值的api序列进行padding\n",
    "#       if len(out_seq_tokens)>api_max_length-1:\n",
    "#          out_seq_tokens=out_seq_tokens[:api_max_length]\n",
    "        process_one_seq(in_seq_tokens,in_tokens,in_seqs,query_max_length)\n",
    "        process_one_seq(out_seq_tokens,out_tokens,out_seqs,api_max_length)\n",
    "    in_vocab,in_data=build_data(in_tokens,in_seqs)\n",
    "    out_vocab,out_data=build_data(out_tokens,out_seqs)\n",
    "    #构建一个词典\n",
    "    for i in range(len(out_data)):\n",
    "         api_dict[tuple(out_data[i].numpy().tolist())] = raw_apis[i] \n",
    "    return in_vocab,out_vocab,Data.TensorDataset(in_data,out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语义相似度GRU模型\n",
    "class GRU(nn.Module):\n",
    "    #其中input_size是指词表的大小\n",
    "    def __init__(self,input_size,hidden_size,out_size):\n",
    "        super(GRU,self).__init__()\n",
    "        \n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding=nn.Embedding(input_size,hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, out_size)\n",
    "        self.state = None\n",
    "    def forward(self,input, state):\n",
    "        # 输入形状是(批量大小, 时间步数)。将输出互换样本维和时间步维\n",
    "        #理解此处的0与1维度互换，因为是GRU序列，按照文本序列一个词一个词的输入进循环网络\n",
    "        embedded=self.embedding(input.long()).permute(1,0,2)# (seq_len, batch, input_size)\n",
    "        output=embedded\n",
    "        output,state=self.gru(output,state)\n",
    "        #移除时间步维，输出形状为(批量大小, 输出词典大小)\n",
    "        #疑问：state是否是最后linear层的输入,输出的尺寸应该为（GRU层数，batch_size，num_hidden）\n",
    "        return self.out(state).squeeze(dim=0)\n",
    "\n",
    "    #def initRNN(self):\n",
    "     #   return None\n",
    "# 例子：创建一个批量大小为4、时间步数为7的小批量序列输入。设门控循环单元的隐藏层个数为2，隐藏单元个数为16。\n",
    "# encoder = Encoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "# output, state = encoder(torch.zeros((4, 7)), encoder.begin_state())\n",
    "# output.shape, state.shape # GRU的state是h, 而LSTM的是一个元组(h, c)\n",
    "\n",
    "# 其中输出形状output为(时间步数, 批量大小, 隐藏单元个数)  torch.Size([7, 4, 16])\n",
    "# 返回的状态为state为(隐藏层个数, 批量大小, 隐藏单元个数) torch.Size([2, 4, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#小批量计算损失\n",
    "def batch_loss(rnn_q,rnn_api,query,api,tag,loss, tmperature = 0.3):\n",
    "    batch_size=query.shape[0]\n",
    "#   初始化GRU的隐藏层状态\n",
    "    q_state=None\n",
    "    api_state=None\n",
    "    q_output=rnn_q(query,q_state)\n",
    "    api_output=rnn_api(api,api_state)\n",
    "    #此时输出的维度猜测应该是（batch_size,output_size）\n",
    "    #下方为AI研习社语义任务之后修改的向量维度\n",
    "    #\n",
    "    # print(q_output.shape)\n",
    "    # print(api_output.shape)\n",
    "    q_output=q_output/torch.norm(q_output,dim=1).unsqueeze(1)\n",
    "    api_output=api_output/torch.norm(api_output,dim=1).unsqueeze(1)\n",
    "    \n",
    "    pre_score=torch.mm(q_output,api_output.permute(1,0))\n",
    "    #此处进行sigmoid方程与结果的概率进行（0-1）匹配。可能存在一部分问题，待确定\n",
    "    score=torch.sigmoid(torch.flatten(pre_score))\n",
    "  #！！！！！这个l的初始化可能有问题！！！！\n",
    "    #tag应该是一个1维向量（由对角矩阵拉直）\n",
    "    l=loss(score / tmperature,tag/tmperature).mean()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练过程\n",
    "def train(rnn_q,rnn_api,dataset,lr,batch_size,num_epochs):\n",
    "    query_optimizer=torch.optim.Adam(rnn_q.parameters(),lr=lr)\n",
    "    api_optimizier=torch.optim.Adam(rnn_api.parameters(),lr=lr)\n",
    "    #将损失函数由交叉熵损失改为MSE损失\n",
    "    loss=nn.MSELoss(reduction='none')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        data_iter=Data.DataLoader(dataset,batch_size,shuffle=True)\n",
    "        ##print(\"epoch为：{}, iter: {}\".format(epoch, len(data_iter)))\n",
    "        l_sum=0.0\n",
    "        for iter, (query,api) in enumerate(data_iter):\n",
    "            query_optimizer.zero_grad()\n",
    "            api_optimizier.zero_grad()\n",
    "            res_tag=torch.flatten(torch.eye(query.shape[0],query.shape[0]))\n",
    "            l=batch_loss(rnn_q,rnn_api,query,api,res_tag,loss)\n",
    "            l.backward()\n",
    "            query_optimizer.step()\n",
    "            api_optimizier.step()\n",
    "            l_sum+=l.item()\n",
    "            if (iter + 1) % 10 == 0:\n",
    "                print(\"iter[{}/{}], loss {:.4f}\".format((iter+1)*batch_size, len(dataset), l_sum / (iter+1)))\n",
    "        print(\"epoch[{}/{}], loss {:.4f}\".format(epoch, num_epochs, l_sum/len(data_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.vocab.Vocab object at 0x000001F95277EBC8>\n",
      "epoch[0/5], loss 2.7468\n",
      "epoch[1/5], loss 2.1441\n",
      "epoch[2/5], loss 2.1109\n",
      "epoch[3/5], loss 2.0978\n",
      "epoch[4/5], loss 2.0948\n"
     ]
    }
   ],
   "source": [
    "#设置相关超参数\n",
    "hidden_size,out_size = 128, 64\n",
    "lr, batch_size, num_epochs =0.01, 8, 5\n",
    "\n",
    "#注意此时 res_tag的形状即为（batch_size,batch_size）\n",
    "\n",
    "\n",
    "#假设此时的batch_size为2，应该是一个对角矩阵（对角线均为1.0）\n",
    "query_max_length=20\n",
    "#需要提前计算api数据集中api序列的最大长度\n",
    "api_max_length=10\n",
    "path=\"./test.txt\"\n",
    "in_vocab,out_vocab,dataset=read_data(path,query_max_length,api_max_length)\n",
    "print(in_vocab)\n",
    "rnn_q = GRU(len(in_vocab),hidden_size,out_size)\n",
    "rnn_api = GRU(len(in_vocab),hidden_size,out_size)\n",
    "train(rnn_q, rnn_api, dataset, lr, batch_size, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.io.Bits#putInt(byte[], int, int)->void\n"
     ]
    }
   ],
   "source": [
    "#收集训练后的API特征向量\n",
    "#注意此时的DataLoader生成的序列应该只包含分类为1的query与api\n",
    "\n",
    "# 注意此时应该重新建立一个dataset,因为我们所需要的api应该是完全的API而不是将各种标点符号删除的训练数据！！！！！\n",
    "data_iter=Data.DataLoader(dataset,batch_size,shuffle=True)\n",
    "\n",
    "def collection(rnn_api,data_iter):\n",
    "    api_output=torch.tensor([0.0]).expand(batch_size,out_size)\n",
    "    #使用apis收集对应的api_idx序列\n",
    "    apis=[]\n",
    "    for query,api in data_iter:\n",
    "    #初始化rnn_api的初始隐层状态\n",
    "#     !!!!注意 apis中添加的应该是一个原生的字符串序列，该功能应该在接下来添加！！！！！！\n",
    "        apis+=api\n",
    "        api_state=None\n",
    "        output=rnn_api(api,api_state)\n",
    "        api_output=torch.cat((api_output,output))\n",
    "    #对生成的向量进行裁剪，去除最开始初始化为零的api_output部分\n",
    "    #   注 意！！！\n",
    "#   需要把输出的api_output初始expand函数产生的全为零的部分清除,之后才可以保存到csv文件中(已裁剪)\n",
    "    api_output=api_output[batch_size:,:]\n",
    "    api_output=api_output/torch.norm(api_output,dim=1).unsqueeze(1)\n",
    "    return api_output,apis\n",
    "\n",
    "\n",
    "#根据build_data函数更改，转化字符串为index序列\n",
    "def build_query(vocab,query_seq):\n",
    "    query_idx=[vocab.stoi[w] for w in query_seq]\n",
    "    return torch.tensor(query_idx)\n",
    "\n",
    "#计算输入查询的index表示并将其输入进模型中生成特征向量，与api序列的特征向量进行查询相似度，输出相似度最高的api接口  \n",
    "def caculate(vocab,api_output,apis,query,query_max_length):\n",
    "    query_seq=query.split(' ')\n",
    "    if len(query_seq)>query_max_length-1:\n",
    "            query_seq=query_seq[:query_max_length-1]\n",
    "    query_data=build_query(vocab,query_seq).unsqueeze(0)\n",
    "    query_state=None\n",
    "    q_output=rnn_q(query_data,query_state)\n",
    "    #q_output形状应为（1，output_size）\n",
    "    q_output=q_output/torch.norm(q_output,dim=1).unsqueeze(1)\n",
    "    #此处传进来的api_output形状应为(所有api个数，output_size)\n",
    "    pre_score=torch.mm(q_output,api_output.permute(1,0))\n",
    "    #此处进行sigmoid方程与结果的概率进行（0-1）匹配。\n",
    "    score=torch.sigmoid(pre_score)\n",
    "    #之后针对分数最高的那个API进行输出序列\n",
    "    _,idx=torch.max(score,1)\n",
    "#     求出最大的那个api index\n",
    "    idx=int(idx)\n",
    "    #根据idx查询出apis中的index，之后通过字典查询出对应的原生字符串\n",
    "    print(api_dict[tuple(apis[idx].numpy().tolist())])\n",
    "\n",
    "#处理query语句使其大转小写\n",
    "query='am scsbh nsjdkvb snvj dscj jsvdb ccc'\n",
    "api_output,apis=collection(rnn_api,data_iter)\n",
    "caculate(in_vocab,api_output,apis,query.lower(),query_max_length)\n",
    "        \n",
    "# 对collection函数进行调用，得到api_output(特征向量)，api（api的字符串序列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下内容是对不熟悉函数的测试，不属于模型的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([10, 11]), tensor([20, 21]), tensor(1))\n",
      "(tensor([12, 13]), tensor([22, 23]), tensor(2))\n"
     ]
    }
   ],
   "source": [
    "# test ：测试Data.TensorDataset类功能\n",
    "# res=torch.tensor([1,2])\n",
    "in1=torch.tensor([[10,11],[12,13]])\n",
    "out=torch.tensor([[20,21],[22,23]])\n",
    "hh=Data.TensorDataset(in1,out,res)\n",
    "for h in hh:\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "torch.Size([2])\n",
      "[[0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec</th>\n",
       "      <th>apiseq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          vec apiseq\n",
       "0  [0.11999999731779099, 0.20000000298023224]   api1\n",
       "1  [0.11999999731779099, 0.20000000298023224]   api2\n",
       "2  [0.11999999731779099, 0.20000000298023224]   api3\n",
       "3  [0.11999999731779099, 0.20000000298023224]   api4\n",
       "4  [0.11999999731779099, 0.20000000298023224]   api5\n",
       "5  [0.11999999731779099, 0.20000000298023224]   api6\n",
       "6  [0.11999999731779099, 0.20000000298023224]   api7\n",
       "7  [0.11999999731779099, 0.20000000298023224]   api8"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test: 测试tensor输出为csv格式的文件\n",
    "\n",
    "import csv\n",
    "\n",
    "d=torch.tensor([0.0])\n",
    "# for i in range(3):\n",
    "out=torch.tensor([[0.12,0.2],[0.12,0.2],[0.12,0.2],[0.12,0.2]])\n",
    "print(out.shape)\n",
    "print(out[0].shape)\n",
    "out=torch.cat((out.expand(out.shape),out))\n",
    "out=out.data.numpy().tolist()\n",
    "print(out)\n",
    "apiseq=['api1','api2','api3','api4','api5','api6','api7','api8']\n",
    "data=pd.DataFrame(columns=['vec','apiseq'])\n",
    "# res=pd.DataFrame(data=out)\n",
    "data['vec'],data['apiseq']=out,apiseq\n",
    "\n",
    "# res.to_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 1., 0., 0.])\n",
      "tensor([0.7311, 0.5000, 0.5000, 0.7311, 0.5000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "#单位矩阵\n",
    "import torch\n",
    "data=torch.flatten(torch.eye(3,2))\n",
    "print(data)\n",
    "data=torch.sigmoid(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "编辑元数据",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda29ca87a343a1497fa9db5a03b8930b5d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
