{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意！！！！！ 需要更改训练函数的维度以及最后两个向量的相似度计算！！！！与AI研习社中的计算方法相同#\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.vocab as vocab\n",
    "import torch.utils.data as Data\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD,EOS='<pad>','<eos>'\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列\n",
    "# 长度变为max_seq_len，然后将序列保存在all_seqs中\n",
    "def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):\n",
    "    all_tokens.extend(seq_tokens)\n",
    "    seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)\n",
    "    all_seqs.append(seq_tokens)\n",
    "\n",
    "# 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造Tensor\n",
    "def build_data(all_tokens, all_seqs):\n",
    "    vocab = Vocab.Vocab(collections.Counter(all_tokens),\n",
    "                        specials=[PAD, EOS])\n",
    "    indices = [[vocab.stoi[w] for w in seq] for seq in all_seqs]\n",
    "    return vocab, torch.tensor(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始读取数据，构建词典\n",
    "def read_data(path,query_max_length,api_max_length):\n",
    "    in_tokens,out_tokens,in_seqs,out_seqs=[],[],[],[]\n",
    "    with io.open(path) as f:\n",
    "        lines=f.readlines;\n",
    "    for line in lines:\n",
    "        #将每一行语句按照'：'将api与对应的query查询分开\n",
    "        in_seq,out_seq,res_tag=line.split(':')\n",
    "        in_seq_tokens,out_seq_tokens=in_seq.split(' '),out_seq.split(' ')\n",
    "        if len(in_seq_tokens)>query_max_length-1:\n",
    "            in_seq_tokens=in_seq_tokens[:query_max_length]\n",
    "        #api序列名数据比较规整可以使用最大长度为api_max_length，对于短于最大值的api序列进行padding\n",
    "#       if len(out_seq_tokens)>api_max_length-1:\n",
    "#          out_seq_tokens=out_seq_tokens[:api_max_length]\n",
    "        process_one_seq(in_seq_tokens,in_tokens,in_seqs,query_max_length)\n",
    "        process_one_seq(out_seq_tokens,out_tokens,out_seqs,api_max_length)\n",
    "    in_vocab,in_data=build_data(in_tokens,in_seqs)\n",
    "    out_vocab,out_data=build_data(out_tokens,out_seqs)\n",
    "    \n",
    "    res_tag=torch.tensor(resTag)  \n",
    "    return in_vocab,out_vocab,Data.TensorDataset(in_data,out_data,res_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语义相似度GRU模型\n",
    "class GRU(nn.Module):\n",
    "    #其中input_size是指词表的大小\n",
    "    def __init__(self,input_size,hidden_size,out_size):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding=nn.Embedding(input_size,hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, out_size)\n",
    "    def forward(self,input,hidden):\n",
    "        # 输入形状是(批量大小, 时间步数)。将输出互换样本维和时间步维\n",
    "        #理解此处的0与1维度互换，因为是GRU序列，按照文本序列一个词一个词的输入进循环网络\n",
    "        embedded=self.embedding(input.long()).permute(1,0,2)# (seq_len, batch, input_size)\n",
    "        output=embedded\n",
    "        output,state=self.gru(output,hidden)\n",
    "        #移除时间步维，输出形状为(批量大小, 输出词典大小)\n",
    "        #疑问：state是否是最后linear层的输入,输出的尺寸应该为（GRU层数，batch_size，num_hidden）\n",
    "        output = self.out(state).squeeze(dim=0)\n",
    "\n",
    "    def initRNN(self):\n",
    "        return None\n",
    "# 例子：创建一个批量大小为4、时间步数为7的小批量序列输入。设门控循环单元的隐藏层个数为2，隐藏单元个数为16。\n",
    "# encoder = Encoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "# output, state = encoder(torch.zeros((4, 7)), encoder.begin_state())\n",
    "# output.shape, state.shape # GRU的state是h, 而LSTM的是一个元组(h, c)\n",
    "\n",
    "# 其中输出形状output为(时间步数, 批量大小, 隐藏单元个数)  torch.Size([7, 4, 16])\n",
    "# 返回的状态为state为(隐藏层个数, 批量大小, 隐藏单元个数) torch.Size([2, 4, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#小批量计算损失\n",
    "def batch_loss(rnn_q,rnn_api,query,api,tag,loss):\n",
    "    batch_size=query.shape[0]\n",
    "#   初始化GRU的隐藏层状态\n",
    "    q_state=rnn_q.initRNN()\n",
    "    api_state=rnn_api.initRNN()\n",
    "    q_output=rnn_q(query,q_state)\n",
    "    api_output=rnn_api(api,api_state)\n",
    "    #此时输出的维度猜测应该是（batch_size,hidden_size）\n",
    "    #下方为AI研习社语义任务之后修改的向量维度\n",
    "    q_output=q_output/torch.norm(q_output,dim=1).unsqueeze(1)\n",
    "    api_output=api_output/torch.norm(api_output,dim),unsqueeze(1)\n",
    "    pre_score=torch.bmm(q_output.unsqueeze(1),api_output.unsqueeze(1).permute(0,2,1))\n",
    "    #此处进行sigmoid方程与结果的概率进行（0-1）匹配。可能存在一部分问题，待确定\n",
    "    score=nn.sigmoid(pre_score)\n",
    "    l=torch.tensor([0.0])\n",
    "    l=loss(score,tag)\n",
    "    return l/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练过程\n",
    "def train(rnn_q,rnn_api,dataset,lr,batch_size,num_epochs):\n",
    "    query_optimizer=torch.optim.Adam(rnn_q.parameters(),lr=lr)\n",
    "    api_optimizier=torch.optim.Adam(rnn_api.parameters(),lr=lr)\n",
    "    #将损失函数由交叉熵损失改为MSE损失\n",
    "    loss=nn.MSELoss(reduction='none')\n",
    "    data_iter=Dara.DataLoader(dataset,batch_size,shuffle=True)\n",
    "    for epoch in ranger(num_epochs):\n",
    "        print(\"第\")+epoch+print(\"次训练开始。。\")\n",
    "        l_sum=0.0\n",
    "        for query,api,tag in data_iter:\n",
    "            query_optimizer.zero_grad()\n",
    "            api_optimizier.zero_grad()\n",
    "            l=batch_loss(rnn_q,rnn_api,query,api,tag,loss)\n",
    "            l.backward()\n",
    "            query_optimizer.step()\n",
    "            api_optimizier.step()\n",
    "            l_sum+=l.item()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"epoch %d, loss %.3f\" % (epoch + 1, l_sum / len(data_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置相关超参数\n",
    "input_size, hidden_size,out_size = 64, 64, 64\n",
    "lr, batch_size, num_epochs =0.01, 2, 50\n",
    "query_max_length=50\n",
    "api_max_length=20\n",
    "path=\"数据集地址\"\n",
    "in_vocab,out_vocab,dataset=read_data(path,query_max_length,api_max_length)\n",
    "rnn_q = GRU(len(in_vocab), input_size, hidden_size,out_size)\n",
    "rnn_api = GRU(len(in_vocab), input_size, hidden_size,out_size)\n",
    "train(rnn_q, rnn_api, dataset, lr, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#收集训练后的API特征向量\n",
    "#注意此时的DataLoader生成的序列应该只包含分类为1的query与api\n",
    "data_iter=Dara.DataLoader(dataset,batch_size,shuffle=True)\n",
    "def collection(rnn_api,data_iter):\n",
    "    api_output=torch.tensor([0.0])\n",
    "    api=[]\n",
    "    for query,api in data_iter:\n",
    "    #初始化rnn_api的初始隐层状态\n",
    "        api=api.append(api)\n",
    "        api_state=rnn_api.initRNN()\n",
    "        output=rnn_api(api,api_state)\n",
    "        api_output=torch.cat((api_output.expand(outpur.shape),output))\n",
    "    return api_output,api\n",
    "\n",
    "#   注 意！！！\n",
    "#   需要把输出的api_output初始expand函数产生的全为零的部分清除,之后才可以保存到csv文件中\n",
    "        \n",
    "# 对collection函数进行调用，得到api_outpur(特征向量)，api（api的字符串序列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下内容是对不熟悉函数的测试，不属于模型的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([10, 11]), tensor([20, 21]), tensor(1))\n",
      "(tensor([12, 13]), tensor([22, 23]), tensor(2))\n"
     ]
    }
   ],
   "source": [
    "# test ：测试Data.TensorDataset类功能\n",
    "# res=torch.tensor([1,2])\n",
    "in1=torch.tensor([[10,11],[12,13]])\n",
    "out=torch.tensor([[20,21],[22,23]])\n",
    "hh=Data.TensorDataset(in1,out,res)\n",
    "for h in hh:\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "torch.Size([2])\n",
      "[[0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224], [0.11999999731779099, 0.20000000298023224]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec</th>\n",
       "      <th>apiseq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.11999999731779099, 0.20000000298023224]</td>\n",
       "      <td>api8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          vec apiseq\n",
       "0  [0.11999999731779099, 0.20000000298023224]   api1\n",
       "1  [0.11999999731779099, 0.20000000298023224]   api2\n",
       "2  [0.11999999731779099, 0.20000000298023224]   api3\n",
       "3  [0.11999999731779099, 0.20000000298023224]   api4\n",
       "4  [0.11999999731779099, 0.20000000298023224]   api5\n",
       "5  [0.11999999731779099, 0.20000000298023224]   api6\n",
       "6  [0.11999999731779099, 0.20000000298023224]   api7\n",
       "7  [0.11999999731779099, 0.20000000298023224]   api8"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test: 测试tensor输出为csv格式的文件\n",
    "\n",
    "import csv\n",
    "\n",
    "d=torch.tensor([0.0])\n",
    "# for i in range(3):\n",
    "out=torch.tensor([[0.12,0.2],[0.12,0.2],[0.12,0.2],[0.12,0.2]])\n",
    "print(out.shape)\n",
    "print(out[0].shape)\n",
    "out=torch.cat((out.expand(out.shape),out))\n",
    "out=out.data.numpy().tolist()\n",
    "print(out)\n",
    "apiseq=['api1','api2','api3','api4','api5','api6','api7','api8']\n",
    "data=pd.DataFrame(columns=['vec','apiseq'])\n",
    "# res=pd.DataFrame(data=out)\n",
    "data['vec'],data['apiseq']=out,apiseq\n",
    "\n",
    "# res.to_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda29ca87a343a1497fa9db5a03b8930b5d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
