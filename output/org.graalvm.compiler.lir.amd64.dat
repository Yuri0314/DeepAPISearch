AMD64AddressValue#getBase()::: d64 address value get base:::return
AMD64AddressValue#getIndex()::: d64 address value get index:::return
AMD64AddressValue#forEachComponent(LIRInstruction, OperandMode, InstructionValueProcedure)::: d64 address value for each component:::newBase->proc->doValue->newIndex->proc->doValue->if->base->identityEquals->index->identityEquals->return->getValueKind->new->AMD64AddressValue->return
AMD64AddressValue#visitEachComponent(LIRInstruction, OperandMode, InstructionValueConsumer)::: d64 address value visit each component:::proc->visitValue->proc->visitValue
AMD64AddressValue#withKind(ValueKind)::: d64 address value with kind:::return->new->AMD64AddressValue
AMD64AddressValue#toRegister(AllocatableValue)::: d64 address value to register:::if->value->equals->return->else->reg->return->reg->getRegister
AMD64AddressValue#toAddress()::: d64 address value to address:::return->toRegister->toRegister->new->AMD64Address
AMD64AddressValue#toString()::: d64 address value to string:::s->new->StringBuilder->sep->if->isLegal->s->append->if->isLegal->s->append->append->append->append->if->s->append->append->else->if->s->append->append->s->append->return->s->toString
AMD64AddressValue#isValidImplicitNullCheckFor(Value, int)::: d64 address value is valid implicit null check for:::return->value->equals->index->equals
AMD64AddressValue#equals(Object)::: d64 address value equals:::if->addr->return->getValueKind->addr->getValueKind->equals->base->equals->index->equals->return
AMD64AddressValue#hashCode()::: d64 address value hash code:::return->base->hashCode->index->hashCode->getValueKind->hashCode
AMD64ArithmeticLIRGeneratorTool#emitCountLeadingZeros(Value)::: d64 arithmetic generator tool emit count leading zeros:::
AMD64ArithmeticLIRGeneratorTool#emitCountTrailingZeros(Value)::: d64 arithmetic generator tool emit count trailing zeros:::
AMD64ArithmeticLIRGeneratorTool#emitLogicalAndNot(Value, Value)::: d64 arithmetic generator tool emit logical and not:::
AMD64ArithmeticLIRGeneratorTool#emitLowestSetIsolatedBit(Value)::: d64 arithmetic generator tool emit lowest set isolated bit:::
AMD64ArithmeticLIRGeneratorTool#emitGetMaskUpToLowestSetBit(Value)::: d64 arithmetic generator tool emit get mask up to lowest set bit:::
AMD64ArithmeticLIRGeneratorTool#emitResetLowestSetBit(Value)::: d64 arithmetic generator tool emit reset lowest set bit:::
AMD64ArithmeticLIRGeneratorTool#emitRound(Value, RoundingMode)::: d64 arithmetic generator tool emit round:::
AMD64ArithmeticLIRGeneratorTool#emitCompareOp(AMD64Kind, Variable, Value)::: d64 arithmetic generator tool emit compare op:::
AMD64ArrayCompareToOp#supportsSSE42(TargetDescription)::: d64 array compare to op supports e42:::arch->return->arch->getFeatures->contains
AMD64ArrayCompareToOp#supportsAVX2(TargetDescription)::: d64 array compare to op supports x2:::arch->return->arch->getFeatures->contains
AMD64ArrayCompareToOp#supportsAVX512VLBW(TargetDescription)::: d64 array compare to op supports x512 w:::features->getFeatures->return->features->contains->features->contains
AMD64ArrayCompareToOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 array compare to op emit code:::result->asRegister->str1->asRegister->str2->asRegister->masm->asRegister->new->AMD64Address->leaq->masm->asRegister->new->AMD64Address->leaq->cnt1->asRegister->cnt2->asRegister->LENGTH_DIFF_LABEL->new->Label->POP_LABEL->new->Label->DONE_LABEL->new->Label->WHILE_HEAD_LABEL->new->Label->COMPARE_WIDE_VECTORS_LOOP_FAILED->new->Label->stride->stride2->adr_stride->adr_stride1->adr_stride2->stride2x2->scale->scale1->scale2->if->if->masm->shrl->masm->movl->masm->subl->masm->push->masm->cmovl->masm->testl->masm->jcc->if->masm->new->AMD64Address->movzbl->masm->new->AMD64Address->movzbl->else->if->masm->new->AMD64Address->movzwl->masm->new->AMD64Address->movzwl->else->masm->new->AMD64Address->movzbl->masm->new->AMD64Address->movzwl->masm->subl->masm->jcc->if->masm->shrl->masm->cmpl->masm->jcc->if->masm->cmpptr->masm->jcc->if->else->else->if->supportsAVX2->supportsSSE42->vec1->asRegister->COMPARE_WIDE_VECTORS->new->Label->VECTOR_NOT_EQUAL->new->Label->COMPARE_WIDE_TAIL->new->Label->COMPARE_SMALL_STR->new->Label->COMPARE_WIDE_VECTORS_LOOP->new->Label->COMPARE_16_CHARS->new->Label->COMPARE_INDEX_CHAR->new->Label->COMPARE_WIDE_VECTORS_LOOP_AVX2->new->Label->COMPARE_TAIL_LONG->new->Label->COMPARE_WIDE_VECTORS_LOOP_AVX3->new->Label->pcmpmask->if->if->else->if->else->masm->movl->masm->andl->masm->jcc->masm->bind->if->masm->new->AMD64Address->movdqu->else->masm->new->AMD64Address->pmovzxbw->masm->new->AMD64Address->pcmpestri->masm->jccb->if->masm->new->AMD64Address->movdqu->masm->new->AMD64Address->pcmpestri->else->masm->new->AMD64Address->pmovzxbw->masm->new->AMD64Address->pcmpestri->masm->jccb->masm->addl->masm->bind->loadNextElements->masm->subl->masm->jmp->masm->bind->if->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->else->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->subl->masm->subl->masm->jcc->masm->negq->masm->bind->if->supportsAVX512VLBW->masm->cmpl->masm->jccb->masm->testl->masm->jccb->masm->bind->if->masm->new->AMD64Address->evmovdqu64->masm->new->AMD64Address->evpcmpeqb->else->masm->new->AMD64Address->evpmovzxbw->masm->new->AMD64Address->evpcmpeqb->masm->kortestq->masm->jcc->masm->addq->masm->subl->masm->jccb->masm->vpxor->masm->jmpb->masm->bind->if->masm->new->AMD64Address->vmovdqu->masm->new->AMD64Address->vpxor->else->masm->new->AMD64Address->vpmovzxbw->masm->new->AMD64Address->vpxor->masm->vptest->masm->jcc->masm->addq->masm->subl->masm->jcc->masm->vpxor->masm->bind->masm->testq->masm->jcc->masm->movl->masm->movl->masm->negq->masm->jmp->masm->bind->masm->vpxor->if->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->else->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->jmp->masm->bind->masm->movl->masm->cmpl->masm->jcc->if->masm->new->AMD64Address->movdqu->else->masm->new->AMD64Address->pmovzxbw->masm->new->AMD64Address->pcmpestri->masm->jcc->masm->subq->masm->jcc->if->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->else->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->masm->jmpb->masm->bind->else->if->supportsSSE42->vec1->asRegister->COMPARE_WIDE_VECTORS->new->Label->VECTOR_NOT_EQUAL->new->Label->COMPARE_TAIL->new->Label->pcmpmask->masm->movl->masm->andl->if->masm->jcc->if->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->else->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->masm->bind->if->masm->new->AMD64Address->movdqu->masm->new->AMD64Address->pcmpestri->else->masm->new->AMD64Address->pmovzxbw->masm->new->AMD64Address->pcmpestri->masm->jccb->masm->addq->masm->subq->masm->jccb->masm->testq->masm->jcc->masm->movl->masm->movl->masm->negq->if->masm->new->AMD64Address->movdqu->masm->new->AMD64Address->pcmpestri->else->masm->new->AMD64Address->pmovzxbw->masm->new->AMD64Address->pcmpestri->masm->jccb->masm->bind->masm->addq->loadNextElements->masm->subl->masm->jmpb->masm->bind->masm->movl->if->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->else->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->decrementl->masm->negq->masm->bind->loadNextElements->masm->subl->masm->jccb->masm->incrementq->masm->jccb->masm->bind->masm->pop->if->masm->sarl->masm->jmpb->if->supportsAVX512VLBW->masm->bind->masm->kmovq->masm->notq->masm->bsfq->if->masm->sarl->masm->addq->if->masm->new->AMD64Address->movzbl->masm->new->AMD64Address->movzbl->else->if->masm->new->AMD64Address->movzwl->masm->new->AMD64Address->movzwl->else->masm->new->AMD64Address->movzwl->masm->new->AMD64Address->movzbl->masm->subl->masm->jmpb->masm->bind->masm->pop->masm->bind->if->masm->negl
AMD64ArrayCompareToOp#loadNextElements(AMD64MacroAssembler, Register, Register, Register, Register, AMD64Address.Scale, AMD64Address.Scale, AMD64Address.Scale, Register)::: d64 array compare to op load next elements:::if->masm->new->AMD64Address->movzbl->masm->new->AMD64Address->movzbl->else->if->masm->new->AMD64Address->movzwl->masm->new->AMD64Address->movzwl->else->masm->new->AMD64Address->movzbl->masm->new->AMD64Address->movzwl
AMD64ArrayEqualsOp#canGenerateConstantLengthCompare(TargetDescription)::: d64 array equals op can generate constant length compare:::return->kind1->isNumericInteger->getElementsPerVector->supportsSSE41
AMD64ArrayEqualsOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 array equals op emit code:::result->asRegister->trueLabel->new->Label->falseLabel->new->Label->done->new->Label->if->canGenerateConstantLengthCompare->new->RegisterArr->asRegister->asRegister->asRegister->asRegister->emitConstantLengthArrayCompareBytes->else->array1->asRegister->array2->asRegister->masm->asRegister->new->AMD64Address->leaq->masm->asRegister->new->AMD64Address->leaq->length->asRegister->masm->asRegister->movl->masm->movl->emitArrayCompare->masm->bind->masm->movl->masm->jmpb->masm->bind->masm->xorl->masm->bind
AMD64ArrayEqualsOp#emitArrayCompare(CompilationResultBuilder, AMD64MacroAssembler, Register, Register, Register, Register, Label, Label)::: d64 array equals op emit array compare:::if->supportsSSE41->emitVectorCompare->if->emit8ByteCompare->emitTailCompares->else->emitDifferentKindsElementWiseCompare
AMD64ArrayEqualsOp#supportsSSE41(TargetDescription):::Returns if the underlying AMD64 architecture supports SSE 4.1 instructions.:::arch->return->arch->getFeatures->contains
AMD64ArrayEqualsOp#emitVectorCompare(CompilationResultBuilder, AMD64MacroAssembler, Register, Register, Register, Register, Label, Label):::Emits code that uses SSE4.1/AVX1 128-bit (16-byte) or AVX2 256-bit (32-byte) vector compares.:::vector1->asRegister->vector2->asRegister->elementsPerVector->getElementsPerVector->loop->new->Label->compareTail->new->Label->requiresNaNCheck->kind1->isNumericFloat->loopCheck->new->Label->nanCheck->new->Label->masm->andl->masm->andl->masm->jcc->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->masm->align->masm->bind->emitVectorLoad1->emitVectorLoad2->emitVectorCmp->masm->jcc->masm->bind->masm->addq->masm->jcc->masm->testl->masm->jcc->if->unalignedCheck->new->Label->masm->jmpb->masm->bind->emitFloatCompareWithinRange->masm->jmpb->masm->bind->vectorSize->getBytes->scaleDisplacement1->emitVectorLoad1->vectorSize->getBytes->scaleDisplacement2->emitVectorLoad2->emitVectorCmp->if->masm->jcc->vectorSize->getBytes->emitFloatCompareWithinRange->else->masm->jcc->masm->jmp->masm->bind->masm->movl
AMD64ArrayEqualsOp#getElementsPerVector(AVXKind.AVXSize)::: d64 array equals op get elements per vector:::return->vSize->getBytes->Math->max
AMD64ArrayEqualsOp#emitVectorLoad1(AMD64MacroAssembler, Register, Register, int, AVXKind.AVXSize)::: d64 array equals op emit vector load1:::emitVectorLoad1
AMD64ArrayEqualsOp#emitVectorLoad2(AMD64MacroAssembler, Register, Register, int, AVXKind.AVXSize)::: d64 array equals op emit vector load2:::emitVectorLoad2
AMD64ArrayEqualsOp#emitVectorLoad1(AMD64MacroAssembler, Register, Register, Register, int, AVXKind.AVXSize)::: d64 array equals op emit vector load1:::emitVectorLoad
AMD64ArrayEqualsOp#emitVectorLoad2(AMD64MacroAssembler, Register, Register, Register, int, AVXKind.AVXSize)::: d64 array equals op emit vector load2:::emitVectorLoad
AMD64ArrayEqualsOp#emitVectorLoad(AMD64MacroAssembler, Register, Register, Register, int, Scale, Scale, AVXKind.AVXSize)::: d64 array equals op emit vector load:::address->new->AMD64Address->if->if->getAVX2LoadAndExtendOp->emit->else->loadAndExtendSSE->else->if->asm->vmovdqu->else->asm->movdqu
AMD64ArrayEqualsOp#scaleDisplacement1(int)::: d64 array equals op scale displacement1:::return->scaleDisplacement
AMD64ArrayEqualsOp#scaleDisplacement2(int)::: d64 array equals op scale displacement2:::return->scaleDisplacement
AMD64ArrayEqualsOp#scaleDisplacement(int, Scale, Scale)::: d64 array equals op scale displacement:::if->return->return
AMD64ArrayEqualsOp#getAVX2LoadAndExtendOp(Scale, Scale, boolean)::: d64 array equals op get x2 load and extend op:::switch->switch->return->return->return->throw->GraalError->shouldNotReachHere->switch->return->return->throw->GraalError->shouldNotReachHere->return->throw->GraalError->shouldNotReachHere
AMD64ArrayEqualsOp#loadAndExtendSSE(AMD64MacroAssembler, Register, AMD64Address, Scale, Scale, boolean)::: d64 array equals op load and extend e:::switch->switch->if->asm->pmovsxbw->else->asm->pmovzxbw->return->if->asm->pmovsxbd->else->asm->pmovzxbd->return->if->asm->pmovsxbq->else->asm->pmovzxbq->return->throw->GraalError->shouldNotReachHere->switch->if->asm->pmovsxwd->else->asm->pmovzxwd->return->if->asm->pmovsxwq->else->asm->pmovzxwq->return->throw->GraalError->shouldNotReachHere->if->asm->pmovsxdq->else->asm->pmovzxdq->return->throw->GraalError->shouldNotReachHere
AMD64ArrayEqualsOp#emitVectorCmp(AMD64MacroAssembler, Register, Register, AVXKind.AVXSize)::: d64 array equals op emit vector cmp:::emitVectorXor->emitVectorTest
AMD64ArrayEqualsOp#emitVectorXor(AMD64MacroAssembler, Register, Register, AVXKind.AVXSize)::: d64 array equals op emit vector xor:::if->masm->vpxor->else->masm->pxor
AMD64ArrayEqualsOp#emitVectorTest(AMD64MacroAssembler, Register, AVXKind.AVXSize)::: d64 array equals op emit vector test:::if->masm->vptest->else->masm->ptest
AMD64ArrayEqualsOp#emit8ByteCompare(CompilationResultBuilder, AMD64MacroAssembler, Register, Register, Register, Register, Label, Label):::Emits code that uses 8-byte vector compares.:::loop->new->Label->compareTail->new->Label->elementsPerVector->requiresNaNCheck->kind1->isNumericFloat->loopCheck->new->Label->nanCheck->new->Label->temp->asRegister->masm->andl->masm->andl->masm->jcc->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->masm->align->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->cmpq->masm->jcc->masm->bind->masm->addq->masm->jccb->masm->testl->masm->jcc->if->unalignedCheck->new->Label->masm->jmpb->masm->bind->for->offset->kind1->getByteCount->masm->jmpb->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->cmpq->if->masm->jcc->for->offset->kind1->getByteCount->else->masm->jccb->masm->jmpb->masm->bind->masm->movl
AMD64ArrayEqualsOp#emitTailCompares(AMD64MacroAssembler, Register, Register, Register, Register, Label, Label):::Emits code to compare the remaining 1 to 4 bytes.:::compare2Bytes->new->Label->compare1Byte->new->Label->temp->asRegister->if->kind1->getByteCount->masm->testl->masm->jccb->masm->new->AMD64Address->movl->masm->new->AMD64Address->cmpl->if->masm->jccb->emitFloatCompare->masm->jmpb->else->masm->jccb->if->kind1->getByteCount->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->bind->masm->testl->masm->jccb->masm->new->AMD64Address->movzwl->masm->new->AMD64Address->movzwl->masm->cmpl->masm->jccb->if->kind1->getByteCount->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->bind->masm->testl->masm->jccb->masm->new->AMD64Address->movzbl->masm->new->AMD64Address->movzbl->masm->cmpl->masm->jccb->else->masm->bind->else->masm->bind
AMD64ArrayEqualsOp#emitDifferentKindsElementWiseCompare(CompilationResultBuilder, AMD64MacroAssembler, Register, Register, Register, Register, Label, Label)::: d64 array equals op emit different kinds element wise compare:::loop->new->Label->compareTail->new->Label->elementsPerLoopIteration->tmp1->asRegister->tmp2->asRegister->masm->andl->masm->andl->masm->jcc->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->masm->xorq->masm->xorq->masm->align->masm->bind->for->i->masm->addq->masm->jccb->masm->bind->masm->testl->masm->jcc->for->i
AMD64ArrayEqualsOp#emitNaNCheck(AMD64MacroAssembler, AMD64Address, Label):::Emits code to fall through if src is NaN, otherwise jump to branchOrdered.:::tempXMMReg->asRegister->if->masm->movflt->else->masm->movdbl->emit->masm->jcc
AMD64ArrayEqualsOp#emitFloatCompare(AMD64MacroAssembler, Register, Register, Register, int, Label, boolean):::Emits code to compare if two floats are bitwise equal or both NaN.:::address1->new->AMD64Address->address2->new->AMD64Address->bitwiseEqual->new->Label->if->temp->asRegister->if->masm->movl->masm->cmpl->else->masm->movq->masm->cmpq->masm->jccb->emitNaNCheck->emitNaNCheck->masm->bind
AMD64ArrayEqualsOp#emitFloatCompareWithinRange(CompilationResultBuilder, AMD64MacroAssembler, Register, Register, Register, int, Label, int):::Emits code to compare float equality within a range.:::loop->new->Label->i->asRegister->masm->movq->masm->negq->masm->align->masm->bind->emitFloatCompare->masm->incrementq->masm->incrementq->masm->jccb->masm->subq
AMD64ArrayEqualsOp#constantLengthCompareNeedsTmpArrayPointers()::: d64 array equals op constant length compare needs tmp array pointers:::vSize->if->getElementsPerVector->vectorCount->getElementsPerVector->return
AMD64ArrayEqualsOp#emitConstantLengthArrayCompareBytes(CompilationResultBuilder, AMD64MacroAssembler, Register[], Label):::Emits specialized assembly for checking equality of memory regions arrayPtr1[0..nBytes] and arrayPtr2[0..nBytes]:::if->return->arrayPtr1->asRegister->arrayPtr2->asRegister->tmp->asRegister->vSize->if->getElementsPerVector->elementsPerVector->getElementsPerVector->if->byteLength->movSize->new->AMD64Address->emitMovBytes->new->AMD64Address->emitXorBytes->asm->jccb->if->new->AMD64Address->emitMovBytes->new->AMD64Address->emitXorBytes->asm->jccb->else->elementsPerVectorLoop->tailCount->vectorCount->bytesPerVector->vSize->getBytes->if->loopBegin->new->Label->tmpArrayPtr1->asRegister->tmpArrayPtr2->asRegister->asm->new->AMD64Address->leaq->asm->new->AMD64Address->leaq->asm->movq->asm->align->asm->bind->emitVectorLoad1->emitVectorLoad2->scaleDisplacement1->emitVectorLoad1->scaleDisplacement2->emitVectorLoad2->emitVectorXor->emitVectorXor->emitVectorTest->asm->jccb->emitVectorTest->asm->jccb->asm->addq->asm->jccb->if->scaleDisplacement1->emitVectorLoad1->scaleDisplacement2->emitVectorLoad2->emitVectorXor->if->emitVectorLoad1->emitVectorLoad2->emitVectorXor->emitVectorTest->asm->jccb->emitVectorTest->asm->jccb
AMD64ArrayEqualsOp#emitMovBytes(AMD64MacroAssembler, Register, AMD64Address, int)::: d64 array equals op emit mov bytes:::switch->if->asm->movsbq->else->asm->movzbq->break->if->asm->movswq->else->asm->movzwq->break->if->asm->movslq->else->asm->movl->break->asm->movq->break->throw->new->IllegalStateException
AMD64ArrayEqualsOp#emitXorBytes(AMD64MacroAssembler, Register, AMD64Address, int)::: d64 array equals op emit xor bytes:::opSize->getOperandSize->XOR->getRMOpcode->emit
AMD64ArrayEqualsOp#getOperandSize(int)::: d64 array equals op get operand size:::switch->return->return->return->return->throw->new->IllegalStateException
AMD64ArrayIndexOfOp#byteMode(JavaKind)::: d64 array index of op byte mode:::return
AMD64ArrayIndexOfOp#charMode(JavaKind)::: d64 array index of op char mode:::return
AMD64ArrayIndexOfOp#getComparisonKind()::: d64 array index of op get comparison kind:::return->byteMode
AMD64ArrayIndexOfOp#getVectorSize()::: d64 array index of op get vector size:::return->AVXKind->getDataSize
AMD64ArrayIndexOfOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 array index of op emit code:::nVectors->arrayPtr->asRegister->arrayLength->asRegister->fromIndex->asRegister->index->asRegister->searchValue->vecCmp->asRegister->asRegister->asRegister->asRegister->vecArray->asRegister->asRegister->asRegister->asRegister->cmpResult->asRegister->asRegister->ret->new->Label->bulkVectorLoop->new->Label->singleVectorLoop->new->Label->vectorFound->new->Label->new->Label->new->Label->new->Label->runVectorized->new->Label->elementWiseLoop->new->Label->elementWiseFound->new->Label->elementWiseNotFound->new->Label->skipBulkVectorLoop->new->Label->vectorSize->getVectorSize->getBytes->valueKind->getByteCount->bulkSize->vectorCompareKind->if->byteMode->asm->new->AMD64Address->leaq->asm->cmpq->asm->jccb->asm->subq->asm->cmpq->asm->jccb->asm->bind->cmpSize->getComparisonKind->getOpSize->arrayAddr->valueKind->getByteCount->new->AMD64Address->valuesOnStack->searchValuesOnStack->if->emit->for->i->else->for->i->asm->incrementq->asm->cmpq->asm->jccb->asm->bind->asm->xorq->if->asm->bind->asm->decrementq->else->asm->decrementq->asm->bind->asm->jmp->asm->bind->for->i->emitVectorCompare->asm->new->AMD64Address->leaq->if->charMode->asm->shrq->asm->addq->asm->andq->asm->subq->asm->addq->asm->cmpq->asm->jccb->emitAlign->asm->bind->emitVectorCompare->asm->addq->asm->cmpq->asm->jccb->asm->bind->if->asm->movq->emitVectorCompare->else->asm->subq->emitAlign->asm->bind->asm->addq->asm->cmpq->asm->cmovq->emitVectorCompare->asm->cmpq->asm->jccb->asm->movl->asm->jmpb->if->vectorFound2Done->new->Label->asm->bind->asm->getResultIndexDelta->subq->asm->jmpb->asm->bind->asm->getResultIndexDelta->subq->asm->bind->asm->bsfq->if->charMode->asm->shrl->asm->addq->asm->jmpb->minResult->new->Label->minResultDone->new->Label->if->asm->bind->asm->getResultIndexDelta->subq->asm->jmpb->asm->bind->asm->getResultIndexDelta->subq->asm->bind->asm->bsfq->asm->testq->asm->jccb->asm->bsfq->asm->valueKind->getByteCount->addq->asm->cmpq->asm->cmovq->asm->bind->if->charMode->asm->shrl->asm->addq->else->end->new->Label->for->i->asm->bind->asm->bsfq->if->charMode->asm->shrl->asm->addq->asm->bind
AMD64ArrayIndexOfOp#searchValuesOnStack(Value[])::: d64 array index of op search values on stack:::for->i->return
AMD64ArrayIndexOfOp#getResultIndexDelta(int)::: d64 array index of op get result index delta:::return->getVectorSize->getBytes->valueKind->getByteCount
AMD64ArrayIndexOfOp#getVectorOffset(int)::: d64 array index of op get vector offset:::return->getResultIndexDelta->valueKind->getByteCount
AMD64ArrayIndexOfOp#broadcastSearchValue(CompilationResultBuilder, AMD64MacroAssembler, Register, Value, Register, Register)::: d64 array index of op broadcast search value:::src->asRegOrTmpReg->if->asm->supports->emit->else->asm->movdl->getComparisonKind->getVectorSize->emitBroadcast
AMD64ArrayIndexOfOp#isConstant(Value)::: d64 array index of op is constant:::return
AMD64ArrayIndexOfOp#asConstant(Value)::: d64 array index of op as constant:::return->getJavaConstant
AMD64ArrayIndexOfOp#asRegOrTmpReg(CompilationResultBuilder, AMD64MacroAssembler, Value, Register)::: d64 array index of op as reg or tmp reg:::if->isRegister->return->asRegister->else->if->isStackSlot->asm->crb->asAddress->movl->return->else->asm->asConstant->asInt->movl->return
AMD64ArrayIndexOfOp#emitAlign(CompilationResultBuilder, AMD64MacroAssembler)::: d64 array index of op emit align:::asm->align
AMD64ArrayIndexOfOp#emitBroadcast(AMD64MacroAssembler, JavaKind, Register, Register, AVXKind.AVXSize):::Fills vecDst with copies of its lowest byte, word or dword.:::switch->if->asm->supports->emit->else->if->asm->supports->emit->emit->else->if->asm->supports->asm->pxor->asm->pshufb->else->asm->punpcklbw->asm->punpcklbw->asm->pshufd->break->if->asm->supports->emit->else->if->asm->supports->emit->emit->else->asm->pshuflw->asm->pshufd->break->if->asm->supports->emit->else->if->asm->supports->emit->else->asm->pshufd->break->throw->new->UnsupportedOperationException
AMD64ArrayIndexOfOp#emitVectorCompare(AMD64MacroAssembler, JavaKind, int, Register, Register, Register[], Register[], Register[], Label[], boolean, boolean)::: d64 array index of op emit vector compare:::for->i->if->for->i->else->for->i
AMD64ArrayIndexOfOp#emitJnz(AMD64MacroAssembler, Register, Label, boolean)::: d64 array index of op emit jnz:::asm->testl->if->asm->jccb->else->asm->jcc
AMD64ArrayIndexOfOp#emitArrayLoad(AMD64MacroAssembler, AVXKind.AVXSize, Register, Register, Register, int, boolean)::: d64 array index of op emit array load:::src->new->AMD64Address->if->asm->supports->loadOp->loadOp->emit->else->asm->movdqu
AMD64ArrayIndexOfOp#emitVectorCompareInst(AMD64MacroAssembler, JavaKind, AVXKind.AVXSize, Register, Register):::Compares all packed bytes/words/dwords in vecArray to vecCmp:::switch->if->asm->supports->emit->else->asm->pcmpeqb->break->if->asm->supports->emit->else->asm->pcmpeqw->break->if->asm->supports->emit->else->asm->pcmpeqd->break->throw->new->UnsupportedOperationException
AMD64ArrayIndexOfOp#emitPOR(AMD64MacroAssembler, AVXKind.AVXSize, Register, Register)::: d64 array index of op emit r:::if->asm->supports->emit->else->asm->por
AMD64ArrayIndexOfOp#emitMOVMSK(AMD64MacroAssembler, AVXKind.AVXSize, Register, Register)::: d64 array index of op emit k:::if->asm->supports->emit->else->asm->pmovmskb
AMD64ArrayIndexOfOp#getOpSize(JavaKind)::: d64 array index of op get op size:::switch->return->return->return->return
AMD64ArrayIndexOfOp#supportsAVX2(LIRGeneratorTool)::: d64 array index of op supports x2:::return->supports
AMD64ArrayIndexOfOp#supports(LIRGeneratorTool, CPUFeature)::: d64 array index of op supports:::return->tool->target->getFeatures->contains
AMD64Binary.TwoOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: two op emit code:::AMD64Move->move->if->isRegister->opcode->asRegister->asRegister->emit->else->opcode->asRegister->crb->asAddress->emit
AMD64Binary.CommutativeTwoOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: commutative two op emit code:::input->if->sameRegister->else->AMD64Move->move->if->isRegister->opcode->asRegister->asRegister->emit->else->opcode->asRegister->crb->asAddress->emit
AMD64Binary.CommutativeTwoOp#getOpcode()::: commutative two op get opcode:::return
AMD64Binary.ConstOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: const op emit code:::AMD64Move->move->opcode->asRegister->emit
AMD64Binary.DataTwoOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: data two op emit code:::AMD64Move->move->opcode->asRegister->crb->recordDataReferenceInCode->emit
AMD64Binary.MemoryTwoOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: memory two op emit code:::AMD64Move->move->if->crb->masm->position->recordImplicitException->opcode->asRegister->y->toAddress->emit
AMD64Binary.MemoryTwoOp#verify()::: memory two op verify:::super->verify
AMD64Binary.MemoryTwoOp#makeNullCheckFor(Value, LIRFrameState, int)::: memory two op make null check for:::if->y->isValidImplicitNullCheckFor->return->return
AMD64Binary.MemoryTwoOp#getOpcode()::: memory two op get opcode:::return
AMD64Binary.RMIOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: op emit code:::if->isRegister->opcode->asRegister->asRegister->emit->else->opcode->asRegister->crb->asAddress->emit
AMD64BinaryConsumer.Op#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: op emit code:::if->isRegister->opcode->asRegister->asRegister->emit->else->opcode->asRegister->crb->asAddress->emit
AMD64BinaryConsumer.ConstOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: const op emit code:::if->isRegister->opcode->asRegister->shouldAnnotate->emit->else->opcode->crb->asAddress->shouldAnnotate->emit
AMD64BinaryConsumer.ConstOp#shouldAnnotate()::: const op should annotate:::return
AMD64BinaryConsumer.ConstOp#getOpcode()::: const op get opcode:::return
AMD64BinaryConsumer.VMConstOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: const op emit code:::crb->recordInlineDataInCode->super->emitCode
AMD64BinaryConsumer.VMConstOp#shouldAnnotate()::: const op should annotate:::return
AMD64BinaryConsumer.DataOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: data op emit code:::opcode->asRegister->crb->recordDataReferenceInCode->emit
AMD64BinaryConsumer.MemoryRMOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: memory op emit code:::if->crb->masm->position->recordImplicitException->opcode->asRegister->y->toAddress->emit
AMD64BinaryConsumer.MemoryRMOp#makeNullCheckFor(Value, LIRFrameState, int)::: memory op make null check for:::if->y->isValidImplicitNullCheckFor->return->return
AMD64BinaryConsumer.MemoryMROp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: memory op emit code:::if->crb->masm->position->recordImplicitException->opcode->x->toAddress->asRegister->emit
AMD64BinaryConsumer.MemoryMROp#makeNullCheckFor(Value, LIRFrameState, int)::: memory op make null check for:::if->x->isValidImplicitNullCheckFor->return->return
AMD64BinaryConsumer.MemoryConstOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: memory const op emit code:::if->crb->masm->position->recordImplicitException->opcode->x->toAddress->shouldAnnotate->emit
AMD64BinaryConsumer.MemoryConstOp#shouldAnnotate()::: memory const op should annotate:::return
AMD64BinaryConsumer.MemoryConstOp#makeNullCheckFor(Value, LIRFrameState, int)::: memory const op make null check for:::if->x->isValidImplicitNullCheckFor->return->return
AMD64BinaryConsumer.MemoryConstOp#getOpcode()::: memory const op get opcode:::return
AMD64BinaryConsumer.MemoryVMConstOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: memory const op emit code:::crb->recordInlineDataInCode->super->emitCode
AMD64BinaryConsumer.MemoryVMConstOp#shouldAnnotate()::: memory const op should annotate:::return
AMD64BlockEndOp#emitCode(CompilationResultBuilder)::: d64 block end op emit code:::emitCode
AMD64BlockEndOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 block end op emit code:::
AMD64BreakpointOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 breakpoint op emit code:::asm->int3
AMD64ByteSwapOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 byte swap op emit code:::AMD64Move->move->switch->input->getPlatformKind->masm->ValueUtil->asRegister->bswapl->break->masm->ValueUtil->asRegister->bswapq->break->throw->GraalError->shouldNotReachHere
AMD64Call.CallOp#destroysCallerSavedRegisters()::: call op destroys caller saved registers:::return
AMD64Call.DirectCallOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: direct call op emit code:::directCall
AMD64Call.DirectCallOp#emitCall(CompilationResultBuilder, AMD64MacroAssembler)::: direct call op emit call:::return->directCall
AMD64Call.IndirectCallOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: indirect call op emit code:::asRegister->indirectCall
AMD64Call.IndirectCallOp#verify()::: indirect call op verify:::super->verify
AMD64Call.ForeignCallOp#destroysCallerSavedRegisters()::: foreign call op destroys caller saved registers:::return->callTarget->destroysRegisters
AMD64Call.DirectNearForeignCallOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: direct near foreign call op emit code:::directCall
AMD64Call.DirectFarForeignCallOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: direct far foreign call op emit code:::getRegister->directCall
AMD64Call#directCall(CompilationResultBuilder, AMD64MacroAssembler, InvokeTarget, Register, boolean, LIRFrameState)::: d64 call direct call:::if->emitAlignmentForDirectCall->before->masm->position->callPCOffset->if->GeneratePIC->crb->getOptions->getValue->masm->movq->masm->position->masm->call->else->masm->position->masm->call->after->masm->position->crb->recordDirectCall->crb->recordExceptionHandlers->masm->ensureUniquePC->return
AMD64Call#emitAlignmentForDirectCall(CompilationResultBuilder, AMD64MacroAssembler)::: d64 call emit alignment for direct call:::offset->masm->position->getMachineCodeCallDisplacementOffset->modulus->if->masm->nop
AMD64Call#directJmp(CompilationResultBuilder, AMD64MacroAssembler, InvokeTarget)::: d64 call direct jmp:::return->directJmp
AMD64Call#directJmp(CompilationResultBuilder, AMD64MacroAssembler, InvokeTarget, Register)::: d64 call direct jmp:::before->masm->position->callPCOffset->if->GeneratePIC->crb->getOptions->getValue->masm->movq->masm->position->masm->jmp->else->masm->position->masm->jmp->after->masm->position->crb->recordDirectCall->masm->ensureUniquePC->return
AMD64Call#directConditionalJmp(CompilationResultBuilder, AMD64MacroAssembler, InvokeTarget, ConditionFlag)::: d64 call direct conditional jmp:::before->masm->position->masm->jcc->after->masm->position->crb->recordDirectCall->masm->ensureUniquePC
AMD64Call#indirectCall(CompilationResultBuilder, AMD64MacroAssembler, Register, InvokeTarget, LIRFrameState)::: d64 call indirect call:::before->masm->position->masm->call->after->masm->position->crb->recordIndirectCall->crb->recordExceptionHandlers->masm->ensureUniquePC->return
AMD64CCall#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 call emit code:::directCall
AMD64CCall#directCall(AMD64MacroAssembler)::: d64 call direct call:::reg->ValueUtil->asRegister->masm->call->masm->ensureUniquePC
AMD64CCall#destroysCallerSavedRegisters()::: d64 call destroys caller saved registers:::return
AMD64ClearRegisterOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 clear register op emit code:::op->asRegister->asRegister->emit
AMD64ControlFlow.ReturnOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: return op emit code:::leave->if->masm->supports->masm->vzeroupper->masm->ret
AMD64ControlFlow.BranchOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: branch op emit code:::isNegated->jccPos->masm->position->if->crb->isSuccessorEdge->jcc->else->if->crb->isSuccessorEdge->jcc->else->if->jcc->masm->trueDestination->label->jmp->else->jcc->masm->falseDestination->label->jmp->crb->recordBranch
AMD64ControlFlow.BranchOp#jcc(AMD64MacroAssembler, boolean, LabelRef)::: branch op jcc:::masm->condition->negate->target->label->jcc
AMD64ControlFlow.FloatBranchOp#jcc(AMD64MacroAssembler, boolean, LabelRef)::: float branch op jcc:::condition->negate->target->label->floatJcc
AMD64ControlFlow.StrategySwitchOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: strategy switch op emit code:::strategy->asRegister->new->SwitchClosure->run
AMD64ControlFlow.StrategySwitchOp.SwitchClosure#emitComparison(Constant)::: switch closure emit comparison:::jc->switch->jc->getJavaKind->lc->jc->asLong->masm->cmpl->break->masm->crb->asLongConstRef->cmpq->break->AMD64Move->asRegister->const2reg->masm->asRegister->cmpptr->break->throw->new->GraalError
AMD64ControlFlow.StrategySwitchOp.SwitchClosure#conditionalJump(int, Condition, Label)::: switch closure conditional jump:::emitComparison->masm->intCond->jcc
AMD64ControlFlow.TableSwitchOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: table switch op emit code:::indexReg->asRegister->idxScratchReg->asRegister->scratchReg->asRegister->if->indexReg->equals->masm->movl->highKey->if->masm->subl->masm->cmpl->else->masm->cmpl->if->masm->defaultTarget->label->jcc->masm->new->AMD64Address->leaq->afterLea->masm->position->masm->new->AMD64Address->movslq->masm->addq->masm->jmp->masm->align->jumpTablePos->masm->position->leaDisplacementPosition->masm->emitInt->foreach->label->target->label->offsetToJumpTableBase->masm->position->if->label->isBound->imm32->label->position->masm->emitInt->else->label->masm->position->addPatchAt->masm->emitByte->masm->emitShort->masm->emitByte->jt->new->JumpTable->addAnnotation
AMD64ControlFlow.HashTableSwitchOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: hash table switch op emit code:::valueReg->asRegister->indexReg->asRegister->scratchReg->asRegister->entryScratchReg->asRegister->masm->new->AMD64Address->leaq->afterLea->masm->position->if->masm->new->AMD64Address->movq->masm->cmpl->masm->defaultTarget->label->jcc->masm->sarq->else->masm->new->AMD64Address->movslq->masm->addq->masm->jmp->if->masm->align->else->masm->align->jumpTablePos->masm->position->leaDisplacementPosition->masm->emitInt->for->i->jt->asInt->asInt->new->JumpTable->addAnnotation
AMD64ControlFlow.CondSetOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: cond set op emit code:::setcc
AMD64ControlFlow.FloatCondSetOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: float cond set op emit code:::setcc
AMD64ControlFlow.CondMoveOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: cond move op emit code:::cmove
AMD64ControlFlow.FloatCondMoveOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: float cond move op emit code:::cmove
AMD64ControlFlow#floatJcc(AMD64MacroAssembler, ConditionFlag, boolean, Label)::: d64 control flow float jcc:::endLabel->new->Label->if->trueOnUnordered->masm->jcc->else->if->trueOnUnordered->masm->jccb->masm->jcc->masm->bind
AMD64ControlFlow#cmove(CompilationResultBuilder, AMD64MacroAssembler, Value, boolean, ConditionFlag, boolean, Value, Value)::: d64 control flow cmove:::AMD64Move->move->cmove->if->if->trueOnUnordered->cmove->else->if->trueOnUnordered->cmove
AMD64ControlFlow#cmove(CompilationResultBuilder, AMD64MacroAssembler, Value, ConditionFlag, Value)::: d64 control flow cmove:::if->isRegister->switch->other->getPlatformKind->masm->asRegister->asRegister->cmovl->break->masm->asRegister->asRegister->cmovq->break->throw->GraalError->shouldNotReachHere->else->addr->crb->asAddress->switch->other->getPlatformKind->masm->asRegister->cmovl->break->masm->asRegister->cmovq->break->throw->GraalError->shouldNotReachHere
AMD64ControlFlow#setcc(AMD64MacroAssembler, Value, ConditionFlag)::: d64 control flow setcc:::switch->result->getPlatformKind->masm->asRegister->setl->break->masm->asRegister->setq->break->throw->GraalError->shouldNotReachHere
AMD64ControlFlow#intCond(Condition)::: d64 control flow int cond:::switch->return->return->return->return->return->return->return->return->return->return->throw->GraalError->shouldNotReachHere
AMD64ControlFlow#floatCond(Condition)::: d64 control flow float cond:::switch->return->return->return->return->return->return->throw->GraalError->shouldNotReachHere
AMD64ControlFlow#trueOnUnordered(Condition)::: d64 control flow true on unordered:::return->floatCond->trueOnUnordered
AMD64ControlFlow#trueOnUnordered(ConditionFlag)::: d64 control flow true on unordered:::switch->return->return->throw->GraalError->shouldNotReachHere
AMD64FrameMap#totalFrameSize()::: d64 frame map total frame size:::result->frameSize->return
AMD64FrameMap#currentFrameSize()::: d64 frame map current frame size:::return->alignFrameSize
AMD64FrameMap#alignFrameSize(int)::: d64 frame map align frame size:::return->NumUtil->getTarget->roundUp
AMD64FrameMap#offsetForStackSlot(StackSlot)::: d64 frame map offset for stack slot:::return->super->offsetForStackSlot
AMD64FrameMap#allocateRBPSpillSlot():::For non-leaf methods, RBP is preserved in the special stack slot required by the HotSpot runtime for walking/inspecting frames of such methods.:::LIRKind->value->allocateSpillSlot->return
AMD64FrameMap#freeRBPSpillSlot()::: d64 frame map free spill slot:::size->LIRKind->value->spillSlotSize
AMD64FrameMap#allocateDeoptimizationRescueSlot()::: d64 frame map allocate deoptimization rescue slot:::return->LIRKind->value->allocateSpillSlot
AMD64FrameMapBuilder#allocateRBPSpillSlot():::For non-leaf methods, RBP is preserved in the special stack slot required by the HotSpot runtime for walking/inspecting frames of such methods.:::return->getFrameMap->allocateRBPSpillSlot
AMD64FrameMapBuilder#freeRBPSpillSlot()::: d64 frame map builder free spill slot:::getFrameMap->freeRBPSpillSlot
AMD64FrameMapBuilder#allocateDeoptimizationRescueSlot()::: d64 frame map builder allocate deoptimization rescue slot:::return->getFrameMap->allocateDeoptimizationRescueSlot
AMD64HotSpotHelper#registersToValues(Register[])::: d64 hot spot helper registers to values:::temps->new->ValueArr->for->i->return
AMD64HotSpotHelper#recordExternalAddress(CompilationResultBuilder, ArrayDataPointerConstant)::: d64 hot spot helper record external address:::return->crb->recordDataReferenceInCode
AMD64HotSpotHelper#pointerConstant(int, int[])::: d64 hot spot helper pointer constant:::return->new->ArrayDataPointerConstant
AMD64LFenceOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 fence op emit code:::asm->lfence
AMD64LIRInstruction#emitCode(CompilationResultBuilder)::: d64 instruction emit code:::emitCode
AMD64LIRInstruction#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 instruction emit code:::
AMD64MathCosOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 math cos op emit code:::block0->new->Label->block1->new->Label->block2->new->Label->block3->new->Label->block4->new->Label->block5->new->Label->block6->new->Label->block7->new->Label->block8->new->Label->block9->new->Label->block10->new->Label->block11->new->Label->block12->new->Label->block13->new->Label->masm->push->masm->subq->masm->new->AMD64Address->movsd->masm->new->AMD64Address->movl->masm->recordExternalAddress->movq->masm->andl->masm->subl->masm->cmpl->masm->jcc->masm->mulsd->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movq->masm->pand->masm->por->masm->addpd->masm->cvttsd2sil->masm->cvtsi2sdl->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movq->masm->mulsd->masm->unpcklpd->masm->addq->masm->movdqu->masm->andq->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->leaq->masm->shlq->masm->addq->masm->mulpd->masm->subsd->masm->recordExternalAddress->mulsd->masm->subsd->masm->new->AMD64Address->movq->masm->unpcklpd->masm->movdqu->masm->subsd->masm->mulpd->masm->subpd->masm->recordExternalAddress->movdqu->masm->mulsd->masm->subsd->masm->mulpd->masm->mulpd->masm->subsd->masm->new->AMD64Address->movdqu->masm->subsd->masm->new->AMD64Address->movq->masm->addsd->masm->subsd->masm->mulsd->masm->mulpd->masm->mulsd->masm->mulpd->masm->mulpd->masm->recordExternalAddress->addpd->masm->new->AMD64Address->mulsd->masm->recordExternalAddress->addpd->masm->mulpd->masm->movdqu->masm->new->AMD64Address->addsd->masm->mulpd->masm->movdqu->masm->addsd->masm->addpd->masm->new->AMD64Address->movq->masm->subsd->masm->subsd->masm->new->AMD64Address->addsd->masm->mulpd->masm->addsd->masm->addsd->masm->addsd->masm->addsd->masm->addsd->masm->unpckhpd->masm->addsd->masm->addsd->masm->jmp->masm->bind->masm->jcc->masm->pextrw->masm->andl->masm->pinsrw->masm->recordExternalAddress->movq->masm->subsd->masm->movdqu->masm->jmp->masm->bind->masm->pextrw->masm->andl->masm->cmpl->masm->jcc->masm->pextrw->masm->andl->masm->subl->masm->shrl->masm->andl->masm->recordExternalAddress->leaq->masm->addq->masm->movdq->masm->new->AMD64Address->movl->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->orl->masm->shrl->masm->movl->masm->imulq->masm->imulq->masm->imulq->masm->new->AMD64Address->movl->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->addq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->movl->masm->imulq->masm->imulq->masm->movl->masm->imulq->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->shlq->masm->orq->masm->imulq->masm->new->AMD64Address->movl->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->movq->masm->imulq->masm->imulq->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->movq->masm->imulq->masm->imulq->masm->shlq->masm->orq->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->imulq->masm->pextrw->masm->recordExternalAddress->leaq->masm->subq->masm->addl->masm->addl->masm->addl->masm->addl->masm->movl->masm->andl->masm->shrl->masm->andl->masm->subl->masm->subl->masm->addq->masm->movl->masm->addl->masm->cmpl->masm->jcc->masm->negl->masm->addl->masm->shll->masm->movl->masm->andl->masm->testl->masm->jcc->masm->shrl->masm->movl->masm->shlq->masm->orq->masm->bind->masm->bind->masm->cmpq->masm->jcc->masm->bind->masm->bsrq->masm->movl->masm->subl->masm->jcc->masm->shlq->masm->movq->masm->shlq->masm->addl->masm->negl->masm->addl->masm->shrq->masm->shrq->masm->orq->masm->orq->masm->bind->masm->cvtsi2sdq->masm->shrq->masm->cvtsi2sdq->masm->xorpd->masm->shll->masm->negl->masm->addl->masm->orl->masm->xorl->masm->pinsrw->masm->recordExternalAddress->movq->masm->recordExternalAddress->movq->masm->xorpd->masm->subl->masm->pinsrw->masm->mulsd->masm->shll->masm->sarl->masm->mulsd->masm->movdqu->masm->mulsd->masm->shrl->masm->addsd->masm->mulsd->masm->addl->masm->xorl->masm->mulsd->masm->movl->masm->addsd->masm->movdqu->masm->addsd->masm->subsd->masm->addsd->masm->bind->masm->recordExternalAddress->movq->masm->mulsd->masm->recordExternalAddress->movq->masm->recordExternalAddress->movq->masm->pand->masm->por->masm->addpd->masm->cvttsd2siq->masm->cvtsi2sdq->masm->recordExternalAddress->movq->masm->recordExternalAddress->movdqu->masm->mulsd->masm->unpcklpd->masm->shll->masm->addl->masm->movdqu->masm->addl->masm->andl->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->leaq->masm->shll->masm->addq->masm->mulpd->masm->subsd->masm->recordExternalAddress->mulsd->masm->subsd->masm->new->AMD64Address->movq->masm->unpcklpd->masm->movdqu->masm->subsd->masm->mulpd->masm->subpd->masm->mulsd->masm->subsd->masm->mulpd->masm->mulpd->masm->subsd->masm->new->AMD64Address->movdqu->masm->subsd->masm->new->AMD64Address->movq->masm->addsd->masm->subsd->masm->subsd->masm->recordExternalAddress->movdqu->masm->mulsd->masm->mulpd->masm->mulsd->masm->mulpd->masm->mulpd->masm->recordExternalAddress->addpd->masm->new->AMD64Address->mulsd->masm->recordExternalAddress->addpd->masm->mulpd->masm->movdqu->masm->new->AMD64Address->addsd->masm->mulpd->masm->movdqu->masm->addsd->masm->addpd->masm->new->AMD64Address->movq->masm->subsd->masm->subsd->masm->new->AMD64Address->addsd->masm->mulpd->masm->addsd->masm->addsd->masm->addsd->masm->addsd->masm->addsd->masm->unpckhpd->masm->movdqu->masm->addsd->masm->addsd->masm->jmp->masm->bind->masm->addl->masm->movq->masm->movq->masm->movl->masm->cmpq->masm->jcc->masm->addl->masm->movq->masm->movq->masm->cmpq->masm->jcc->masm->xorpd->masm->xorpd->masm->jmp->masm->bind->masm->jcc->masm->negl->masm->shrq->masm->movq->masm->shrq->masm->subl->masm->negl->masm->addl->masm->shlq->masm->orq->masm->jmp->masm->bind->masm->negl->masm->shlq->masm->orq->masm->shlq->masm->movq->masm->testl->masm->jcc->masm->shrl->masm->movl->masm->shrq->masm->jmp->masm->bind->masm->shrl->masm->movl->masm->shrl->masm->shlq->masm->orq->masm->shlq->masm->addl->masm->movl->masm->movl->masm->subq->masm->sbbq->masm->sbbq->masm->movq->masm->movq->masm->movq->masm->movl->masm->jmp->masm->bind->masm->shrl->masm->movq->masm->shrq->masm->movl->masm->movl->masm->subq->masm->sbbq->masm->sbbq->masm->movq->masm->movq->masm->movq->masm->movl->masm->shrq->masm->addl->masm->jmp->masm->bind->masm->new->AMD64Address->movsd->masm->recordExternalAddress->mulsd->masm->new->AMD64Address->movq->masm->bind->masm->addq->masm->pop
AMD64MathExpOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 math exp op emit code:::block0->new->Label->block1->new->Label->block2->new->Label->block3->new->Label->block4->new->Label->block5->new->Label->block6->new->Label->block7->new->Label->block8->new->Label->block9->new->Label->block10->new->Label->block11->new->Label->block12->new->Label->block13->new->Label->masm->subq->masm->new->AMD64Address->movsd->masm->unpcklpd->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movdqu->masm->pextrw->masm->andl->masm->movl->masm->subl->masm->subl->masm->orl->masm->cmpl->masm->jcc->masm->mulpd->masm->addpd->masm->movapd->masm->subpd->masm->mulpd->masm->recordExternalAddress->movdqu->masm->mulpd->masm->recordExternalAddress->movdqu->masm->subpd->masm->movdl->masm->movl->masm->andl->masm->shll->masm->sarl->masm->movl->masm->recordExternalAddress->movdqu->masm->pand->masm->recordExternalAddress->movdqu->masm->paddq->masm->psllq->masm->subpd->masm->recordExternalAddress->leaq->masm->new->AMD64Address->movdqu->masm->mulpd->masm->movapd->masm->movapd->masm->mulpd->masm->mulpd->masm->addpd->masm->mulsd->masm->recordExternalAddress->mulpd->masm->addsd->masm->unpckhpd->masm->mulpd->masm->addsd->masm->por->masm->unpckhpd->masm->addsd->masm->addsd->masm->addl->masm->cmpl->masm->jcc->masm->mulsd->masm->addsd->masm->jmp->masm->bind->masm->xorpd->masm->recordExternalAddress->movdqu->masm->movl->masm->subl->masm->movdl->masm->psllq->masm->movl->masm->sarl->masm->pinsrw->masm->recordExternalAddress->movdqu->masm->psllq->masm->psubd->masm->mulsd->masm->cmpl->masm->jcc->masm->pand->masm->paddd->masm->subsd->masm->addsd->masm->cmpl->masm->jcc->masm->pextrw->masm->andl->masm->orl->masm->cmpl->masm->jcc->masm->movapd->masm->addsd->masm->mulsd->masm->pextrw->masm->andl->masm->cmpl->masm->jcc->masm->jmp->masm->bind->masm->mulsd->masm->mulsd->masm->movdqu->masm->pxor->masm->psrad->masm->pshufd->masm->psllq->masm->psrlq->masm->pxor->masm->psrlq->masm->paddq->masm->paddq->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->addsd->masm->mulsd->masm->jmp->masm->bind->masm->addsd->masm->mulsd->masm->pextrw->masm->andl->masm->cmpl->masm->jcc->masm->jmp->masm->bind->masm->paddd->masm->addpd->masm->mulsd->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->cmpl->masm->jcc->masm->new->AMD64Address->movl->masm->cmpl->masm->jcc->masm->recordExternalAddress->movsd->masm->mulsd->masm->bind->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->recordExternalAddress->movsd->masm->mulsd->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->new->AMD64Address->movl->masm->cmpl->masm->jcc->masm->cmpl->masm->jcc->masm->new->AMD64Address->movl->masm->cmpl->masm->jcc->masm->recordExternalAddress->movsd->masm->jmp->masm->bind->masm->recordExternalAddress->movsd->masm->jmp->masm->bind->masm->new->AMD64Address->movsd->masm->addsd->masm->jmp->masm->bind->masm->new->AMD64Address->movl->masm->andl->masm->cmpl->masm->jcc->masm->new->AMD64Address->movsd->masm->recordExternalAddress->addsd->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->movq->masm->bind->masm->addq
AMD64MathIntrinsicBinaryOp#emitLIRWrapper(LIRGenerator, Value, Value)::: d64 math intrinsic binary op emit wrapper:::kind->LIRKind->combine->xmm0Value->xmm0->asValue->gen->emitMove->xmm1Value->xmm1->asValue->gen->emitMove->gen->append->result->gen->newVariable->gen->emitMove->return
AMD64MathIntrinsicUnaryOp#emitLIRWrapper(LIRGeneratorTool, Value)::: d64 math intrinsic unary op emit wrapper:::kind->LIRKind->combine->xmm0Value->xmm0->asValue->gen->emitMove->gen->append->result->gen->newVariable->gen->emitMove->return
AMD64MathLog10Op#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 math log10 op emit code:::block0->new->Label->block1->new->Label->block2->new->Label->block3->new->Label->block4->new->Label->block5->new->Label->block6->new->Label->block7->new->Label->block8->new->Label->block9->new->Label->masm->subq->masm->new->AMD64Address->movsd->masm->xorpd->masm->movl->masm->pinsrw->masm->movl->masm->movdl->masm->xorpd->masm->movl->masm->pinsrw->masm->movdqu->masm->movl->masm->movdl->masm->recordExternalAddress->movdqu->masm->pextrw->masm->por->masm->movl->masm->psrlq->masm->recordExternalAddress->movdqu->masm->psrld->masm->rcpps->masm->psllq->masm->pshufd->masm->psrlq->masm->subl->masm->cmpl->masm->jcc->masm->bind->masm->mulss->masm->por->masm->recordExternalAddress->leaq->masm->andpd->masm->paddd->masm->subsd->masm->movdl->masm->psllq->masm->andpd->masm->andl->masm->subl->masm->cvtsi2sdl->masm->mulpd->masm->mulsd->masm->recordExternalAddress->movq->masm->recordExternalAddress->movdqu->masm->subsd->masm->andl->masm->shrl->masm->new->AMD64Address->movdqu->masm->recordExternalAddress->movdqu->masm->addsd->masm->recordExternalAddress->movdqu->masm->mulsd->masm->pshufd->masm->recordExternalAddress->mulsd->masm->mulsd->masm->addsd->masm->mulpd->masm->recordExternalAddress->movq->masm->mulpd->masm->addpd->masm->mulpd->masm->pshufd->masm->addsd->masm->mulsd->masm->subsd->masm->mulsd->masm->addsd->masm->pshufd->masm->mulsd->masm->addsd->masm->addsd->masm->addpd->masm->addsd->masm->mulpd->masm->addsd->masm->pshufd->masm->addsd->masm->addsd->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->movq->masm->addl->masm->cmpl->masm->jcc->masm->cmpl->masm->jcc->masm->bind->masm->addsd->masm->jmp->masm->bind->masm->jcc->masm->cmpl->masm->jcc->masm->jmp->masm->bind->masm->xorpd->masm->addsd->masm->movdl->masm->psrlq->masm->movdl->masm->orl->masm->cmpl->masm->jcc->masm->xorpd->masm->movl->masm->pinsrw->masm->mulsd->masm->xorpd->masm->movl->masm->pinsrw->masm->movdqu->masm->pextrw->masm->por->masm->movl->masm->psrlq->masm->recordExternalAddress->movdqu->masm->psrld->masm->rcpps->masm->psllq->masm->pshufd->masm->psrlq->masm->jmp->masm->bind->masm->movdl->masm->psrlq->masm->movdl->masm->addl->masm->cmpl->masm->jcc->masm->orl->masm->cmpl->masm->jcc->masm->bind->masm->xorpd->masm->xorpd->masm->movl->masm->pinsrw->masm->mulsd->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->xorpd->masm->xorpd->masm->movl->masm->pinsrw->masm->divsd->masm->new->AMD64Address->movl->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->movq->masm->bind->masm->addq
AMD64MathLogOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 math log op emit code:::block0->new->Label->block1->new->Label->block2->new->Label->block3->new->Label->block4->new->Label->block5->new->Label->block6->new->Label->block7->new->Label->block8->new->Label->block9->new->Label->masm->subq->masm->new->AMD64Address->movsd->masm->movq->masm->movdq->masm->movq->masm->movdq->masm->movl->masm->movdl->masm->movq->masm->movdq->masm->movdqu->masm->pextrw->masm->por->masm->movl->masm->psrlq->masm->recordExternalAddress->leaq->masm->psrld->masm->rcpps->masm->psllq->masm->pshufd->masm->psrlq->masm->subl->masm->cmpl->masm->jcc->masm->bind->masm->paddd->masm->por->masm->movdl->masm->psllq->masm->pand->masm->pand->masm->subsd->masm->mulpd->masm->andl->masm->subl->masm->cvtsi2sdl->masm->mulsd->masm->recordExternalAddress->movq->masm->recordExternalAddress->movdqu->masm->subsd->masm->andl->masm->shrl->masm->new->AMD64Address->movdqu->masm->recordExternalAddress->movdqu->masm->addsd->masm->recordExternalAddress->movdqu->masm->mulsd->if->masm->supports->masm->movddup->else->masm->movdqu->masm->movlhps->masm->recordExternalAddress->mulsd->masm->mulsd->masm->addsd->masm->mulpd->masm->mulpd->if->masm->supports->masm->movddup->else->masm->movdqu->masm->movlhps->masm->addsd->masm->addpd->masm->mulpd->masm->subsd->masm->mulsd->masm->pshufd->masm->addsd->masm->mulsd->masm->addsd->masm->addpd->masm->addsd->masm->mulpd->masm->addsd->masm->pshufd->masm->addsd->masm->addsd->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->movq->masm->addl->masm->cmpl->masm->jcc->masm->cmpl->masm->jcc->masm->bind->masm->addsd->masm->jmp->masm->bind->masm->jcc->masm->cmpl->masm->jcc->masm->jmp->masm->bind->masm->xorpd->masm->addsd->masm->movdl->masm->psrlq->masm->movdl->masm->orl->masm->cmpl->masm->jcc->masm->xorpd->masm->movl->masm->pinsrw->masm->mulsd->masm->movdqu->masm->pextrw->masm->por->masm->psrlq->masm->movl->masm->psrld->masm->rcpps->masm->psllq->masm->pshufd->masm->psrlq->masm->jmp->masm->bind->masm->movdl->masm->psrlq->masm->movdl->masm->addl->masm->cmpl->masm->jcc->masm->orl->masm->cmpl->masm->jcc->masm->bind->masm->xorpd->masm->xorpd->masm->movl->masm->pinsrw->masm->mulsd->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->xorpd->masm->xorpd->masm->movl->masm->pinsrw->masm->divsd->masm->new->AMD64Address->movl->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->movq->masm->bind->masm->addq
AMD64MathPowOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 math pow op emit code:::block0->new->Label->block1->new->Label->block2->new->Label->block3->new->Label->block4->new->Label->block5->new->Label->block6->new->Label->block7->new->Label->block8->new->Label->block9->new->Label->block10->new->Label->block11->new->Label->block12->new->Label->block13->new->Label->block14->new->Label->block15->new->Label->block16->new->Label->block17->new->Label->block18->new->Label->block19->new->Label->block20->new->Label->block21->new->Label->block22->new->Label->block23->new->Label->block24->new->Label->block25->new->Label->block26->new->Label->block27->new->Label->block28->new->Label->block29->new->Label->block30->new->Label->block31->new->Label->block32->new->Label->block33->new->Label->block34->new->Label->block35->new->Label->block36->new->Label->block37->new->Label->block38->new->Label->block39->new->Label->block40->new->Label->block41->new->Label->block42->new->Label->block43->new->Label->block44->new->Label->block45->new->Label->block46->new->Label->block47->new->Label->block48->new->Label->block49->new->Label->block50->new->Label->block51->new->Label->block52->new->Label->block53->new->Label->block54->new->Label->block55->new->Label->block56->new->Label->block57->new->Label->tmp1->tmp2->tmp3->tmp4->masm->subq->masm->new->AMD64Address->movsd->masm->new->AMD64Address->movsd->masm->movdq->masm->recordExternalAddress->cmpq->masm->jccb->masm->mulsd->masm->jmp->masm->bind->masm->pextrw->masm->xorpd->masm->movq->masm->movdq->masm->movl->masm->movdq->masm->xorpd->masm->movq->masm->movdq->masm->movdqu->masm->movl->masm->andl->masm->subl->masm->movl->masm->sarl->masm->addl->masm->xorl->masm->por->masm->recordExternalAddress->movdqu->masm->psrlq->masm->recordExternalAddress->movq->masm->psrld->masm->addl->masm->bsrl->masm->rcpps->masm->psllq->masm->movl->masm->movdq->masm->psrlq->masm->subl->masm->cmpl->masm->jcc->masm->movl->masm->bind->masm->mulss->masm->movl->masm->subl->masm->shll->masm->shlq->masm->movdq->masm->por->masm->subl->masm->cmpl->masm->jcc->masm->paddd->masm->pand->masm->movdl->masm->psllq->masm->bind->masm->subsd->masm->pand->masm->subl->masm->sarl->masm->cvtsi2sdl->masm->mulpd->masm->bind->masm->mulsd->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->leaq->masm->subsd->masm->recordExternalAddress->movdqu->masm->movl->masm->sarl->masm->addl->masm->xorl->masm->addl->masm->bsrl->masm->unpcklpd->masm->recordExternalAddress->movdqu->masm->addsd->masm->andl->masm->shrl->masm->new->AMD64Address->addpd->masm->recordExternalAddress->movdqu->masm->pshufd->masm->mulsd->masm->mulpd->masm->mulpd->masm->addsd->masm->mulsd->masm->addpd->masm->mulsd->masm->addpd->masm->new->AMD64Address->movq->masm->new->AMD64Address->movw->masm->pshufd->masm->recordExternalAddress->movq->masm->mulpd->masm->pshufd->masm->mulpd->masm->shll->masm->subl->masm->andl->masm->addl->masm->mulpd->masm->cmpl->masm->jcc->masm->xorpd->masm->movl->masm->pinsrw->masm->movdqu->masm->pand->masm->subsd->masm->mulsd->masm->addsd->masm->mulsd->masm->movdqu->masm->addsd->masm->recordExternalAddress->leaq->masm->addpd->masm->movdl->masm->subsd->masm->pshufd->masm->subsd->masm->addsd->masm->movl->masm->andl->masm->addl->masm->new->AMD64Address->movdqu->masm->addsd->masm->mulsd->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movdqu->masm->shll->masm->xorl->masm->andl->masm->movdq->masm->addsd->masm->movq->masm->movdq->masm->pshufd->masm->pshufd->masm->mulsd->masm->pshufd->masm->mulpd->masm->mulpd->masm->paddd->masm->mulsd->masm->pshufd->masm->mulsd->masm->addpd->masm->addsd->masm->mulpd->masm->pshufd->masm->mulsd->masm->mulsd->masm->addsd->masm->addsd->masm->addsd->masm->jmp->masm->bind->masm->addl->masm->movl->masm->andl->masm->cmpl->masm->jcc->masm->testl->masm->jcc->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->movq->masm->movdl->masm->psrlq->masm->movdl->masm->orl->masm->cmpl->masm->jcc->masm->xorpd->masm->movl->masm->pinsrw->masm->mulsd->masm->xorpd->masm->movl->masm->pinsrw->masm->movdqu->masm->pextrw->masm->por->masm->movl->masm->psrlq->masm->recordExternalAddress->movq->masm->psrld->masm->rcpps->masm->psllq->masm->recordExternalAddress->movdqu->masm->psrlq->masm->mulss->masm->movl->masm->movdl->masm->por->masm->paddd->masm->psllq->masm->movdl->masm->psllq->masm->pand->masm->movl->masm->pand->masm->subsd->masm->andl->masm->subl->masm->sarl->masm->cvtsi2sdl->masm->mulpd->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->movq->masm->movdl->masm->psrlq->masm->movdl->masm->orl->masm->cmpl->masm->jcc->masm->xorpd->masm->movl->masm->pinsrw->masm->mulsd->masm->xorpd->masm->movl->masm->pinsrw->masm->movdqu->masm->pextrw->masm->por->masm->movl->masm->psrlq->masm->recordExternalAddress->movq->masm->psrld->masm->rcpps->masm->psllq->masm->recordExternalAddress->movdqu->masm->psrlq->masm->mulss->masm->movl->masm->movdl->masm->por->masm->paddd->masm->psllq->masm->movdl->masm->psllq->masm->pand->masm->movl->masm->pand->masm->subsd->masm->andl->masm->subl->masm->sarl->masm->cvtsi2sdl->masm->mulpd->masm->jmp->masm->bind->masm->cmpl->masm->jcc->masm->cmpl->masm->jcc->masm->addsd->masm->recordExternalAddress->movq->masm->addpd->masm->xorpd->masm->movl->masm->pinsrw->masm->pshufd->masm->addsd->masm->movdqu->masm->addsd->masm->movdqu->masm->subsd->masm->movdqu->masm->pand->masm->movdqu->masm->pand->masm->subsd->masm->addsd->masm->subsd->masm->mulsd->masm->addsd->masm->mulsd->masm->movdqu->masm->mulsd->masm->addsd->masm->movdl->masm->subsd->masm->recordExternalAddress->leaq->masm->addsd->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movdqu->masm->subsd->masm->pextrw->masm->movl->masm->andl->masm->addl->masm->new->AMD64Address->movdqu->masm->addsd->masm->sarl->masm->movl->masm->sarl->masm->subl->masm->shll->masm->xorl->masm->movdl->masm->recordExternalAddress->movq->masm->andl->masm->cmpl->masm->jcc->masm->pshufd->masm->pshufd->masm->mulpd->masm->mulpd->masm->pshufd->masm->mulsd->masm->mulsd->masm->paddd->masm->addpd->masm->mulsd->masm->pshufd->masm->mulpd->masm->addsd->masm->pshufd->masm->mulsd->masm->mulsd->masm->shll->masm->xorpd->masm->addl->masm->pinsrw->masm->addsd->masm->addsd->masm->movdqu->masm->addsd->masm->mulsd->masm->pextrw->masm->andl->masm->jcc->masm->cmpl->masm->jcc->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->movq->masm->movdqu->masm->movdl->masm->psrlq->masm->movdl->masm->orl->masm->jcc->masm->movdl->masm->psrlq->masm->movdl->masm->movl->masm->addl->masm->orl->masm->jcc->masm->addsd->masm->jmp->masm->bind->masm->xorpd->masm->movl->masm->pinsrw->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->addpd->masm->jmp->masm->bind->masm->movdl->masm->movdqu->masm->psrlq->masm->movdl->masm->movl->masm->addl->masm->orl->masm->jcc->masm->pextrw->masm->andl->masm->cmpl->masm->jcc->masm->movdl->masm->psrlq->masm->movdl->masm->orl->masm->jcc->masm->bind->masm->pextrw->masm->testl->masm->jcc->masm->testl->masm->jcc->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->movdl->masm->testl->masm->jcc->masm->testl->masm->jcc->masm->jmp->masm->bind->masm->shrl->masm->andl->masm->cmpl->masm->jcc->masm->jcc->masm->cmpl->masm->jcc->masm->cmpl->masm->jcc->masm->new->AMD64Address->movq->masm->movl->masm->xorpd->masm->pinsrw->masm->movdqu->masm->addsd->masm->subsd->masm->addsd->masm->pextrw->masm->andl->masm->jcc->masm->movdl->masm->andl->masm->jcc->masm->bind->masm->new->AMD64Address->movq->masm->pextrw->masm->andl->masm->jcc->masm->jmp->masm->bind->masm->xorpd->masm->movl->masm->pinsrw->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->pextrw->masm->andl->masm->jcc->masm->xorpd->masm->movl->masm->pinsrw->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->movdl->masm->andl->masm->jcc->masm->jmp->masm->bind->masm->movdl->masm->psrlq->masm->movdl->masm->orl->masm->jcc->masm->new->AMD64Address->movq->masm->addsd->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->pextrw->masm->cmpl->masm->jcc->masm->movdl->masm->psrlq->masm->movdl->masm->orl->masm->jcc->masm->xorpd->masm->movl->masm->pinsrw->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->andl->masm->subl->masm->pextrw->masm->xorpd->masm->xorl->masm->andl->masm->jcc->masm->jmp->masm->bind->masm->movl->masm->pinsrw->masm->jmp->masm->bind->masm->movdl->masm->cmpl->masm->jcc->masm->testl->masm->jcc->masm->testl->masm->jcc->masm->jmp->masm->bind->masm->testl->masm->jcc->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->movdl->masm->psrlq->masm->movdl->masm->orl->masm->jcc->masm->new->AMD64Address->movq->masm->pextrw->masm->movdl->masm->movdqu->masm->psrlq->masm->movdl->masm->addl->masm->orl->masm->jcc->masm->andl->masm->cmpl->masm->jcc->masm->cmpl->masm->jcc->masm->cmpl->masm->jcc->masm->cmpl->masm->jcc->masm->movl->masm->xorpd->masm->pinsrw->masm->movdqu->masm->addsd->masm->subsd->masm->addsd->masm->pextrw->masm->andl->masm->jcc->masm->movdl->masm->andl->masm->jcc->masm->bind->masm->xorpd->masm->movl->masm->pinsrw->masm->recordExternalAddress->movq->masm->new->AMD64Address->movq->masm->pextrw->masm->movl->masm->movdl->masm->andl->masm->subl->masm->jcc->masm->movl->masm->andl->masm->subl->masm->movl->masm->sarl->masm->addl->masm->xorl->masm->addl->masm->bsrl->masm->movl->masm->jmp->masm->bind->masm->xorpd->masm->movl->masm->pinsrw->masm->xorpd->masm->mulsd->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->xorpd->masm->movl->masm->pinsrw->masm->recordExternalAddress->movq->masm->new->AMD64Address->movq->masm->pextrw->masm->movl->masm->movdl->masm->andl->masm->subl->masm->jcc->masm->movl->masm->andl->masm->subl->masm->movl->masm->sarl->masm->addl->masm->xorl->masm->addl->masm->bsrl->masm->movl->masm->jmp->masm->bind->masm->xorpd->masm->movl->masm->pinsrw->masm->jmp->masm->bind->masm->xorpd->masm->jmp->masm->bind->masm->addl->masm->cmpl->masm->jcc->masm->mulsd->masm->addsd->masm->shrl->masm->addpd->masm->pshufd->masm->addsd->masm->recordExternalAddress->leaq->masm->new->AMD64Address->movq->masm->mulsd->masm->xorpd->masm->movl->masm->shll->masm->orl->masm->pinsrw->masm->addsd->masm->mulsd->masm->addsd->masm->jmp->masm->bind->masm->bind->masm->xorpd->masm->movl->masm->pinsrw->masm->jmp->masm->bind->masm->xorpd->masm->movl->masm->pinsrw->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->movdqu->masm->pextrw->masm->andl->masm->cmpl->masm->jcc->masm->movdl->masm->psrlq->masm->movdl->masm->orl->masm->jcc->masm->bind->masm->movdl->masm->psrlq->masm->movdl->masm->movl->masm->addl->masm->orl->masm->jcc->masm->shrl->masm->cmpl->masm->jcc->masm->jcc->masm->cmpl->masm->jcc->masm->new->AMD64Address->movq->masm->movl->masm->xorpd->masm->pinsrw->masm->movdqu->masm->addsd->masm->subsd->masm->addsd->masm->pextrw->masm->andl->masm->jcc->masm->movdl->masm->andl->masm->jcc->masm->bind->masm->new->AMD64Address->movq->masm->testl->masm->jcc->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->movdl->masm->testl->masm->jcc->masm->bind->masm->testl->masm->jcc->masm->xorpd->masm->bind->masm->movl->masm->xorpd->masm->pinsrw->masm->divsd->masm->movdqu->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->movq->masm->pextrw->masm->pextrw->masm->movl->masm->andl->masm->cmpl->masm->jcc->masm->andl->masm->subl->masm->xorl->masm->testl->masm->jcc->masm->bind->masm->movl->masm->pinsrw->masm->shrl->masm->orl->masm->pinsrw->masm->mulsd->masm->bind->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->movl->masm->pinsrw->masm->mulsd->masm->testl->masm->jcc->masm->movq->masm->movdq->masm->xorpd->masm->bind->masm->new->AMD64Address->movl->masm->jmp->masm->bind->masm->pextrw->masm->pextrw->masm->movl->masm->andl->masm->subl->masm->andl->masm->addl->masm->movl->masm->sarl->masm->subl->masm->jcc->masm->cmpl->masm->jcc->masm->shll->masm->bind->masm->movdl->masm->psllq->masm->pand->masm->subsd->masm->addsd->masm->mulsd->masm->mulsd->masm->addsd->masm->bind->masm->jmp->masm->bind->masm->new->AMD64Address->movw->masm->movl->masm->movdl->masm->xorpd->masm->paddd->masm->movdl->masm->psllq->masm->paddq->masm->pand->masm->andl->masm->cmpl->masm->jcc->masm->pand->masm->subsd->masm->addl->masm->shrl->masm->subl->masm->cvtsi2sdl->masm->mulpd->masm->recordExternalAddress->leaq->masm->recordExternalAddress->movq->masm->mulsd->masm->recordExternalAddress->movq->masm->subsd->masm->recordExternalAddress->movq->masm->pshufd->masm->unpcklpd->masm->addsd->masm->recordExternalAddress->movq->masm->andl->masm->shrl->masm->new->AMD64Address->addpd->masm->mulsd->masm->mulsd->masm->mulsd->masm->mulsd->masm->movdqu->masm->mulsd->masm->addsd->masm->movdqu->masm->addsd->masm->addsd->masm->mulsd->masm->subsd->masm->movdqu->masm->addsd->masm->addsd->masm->subsd->masm->addsd->masm->pshufd->masm->movdqu->masm->addsd->masm->addsd->masm->recordExternalAddress->movdqu->masm->subsd->masm->addsd->masm->movdqu->masm->addsd->masm->addsd->masm->recordExternalAddress->movdqu->masm->subsd->masm->addsd->masm->addsd->masm->pshufd->masm->movapd->masm->addsd->masm->subsd->masm->addsd->masm->recordExternalAddress->movdqu->masm->pshufd->masm->addsd->masm->addsd->masm->recordExternalAddress->movdqu->masm->mulpd->masm->mulpd->masm->pshufd->masm->mulpd->masm->addpd->masm->addpd->masm->mulsd->masm->recordExternalAddress->movq->masm->mulpd->masm->new->AMD64Address->movq->masm->new->AMD64Address->movw->masm->mulpd->masm->pextrw->masm->mulpd->masm->mulpd->masm->recordExternalAddress->movq->masm->pand->masm->addsd->masm->subsd->masm->addpd->masm->andl->masm->subl->masm->andl->masm->cmpl->masm->jcc->masm->addl->masm->cmpl->masm->jcc->masm->pshufd->masm->pand->masm->movdqu->masm->addsd->masm->subsd->masm->xorpd->masm->movl->masm->pinsrw->masm->addsd->masm->mulsd->masm->mulsd->masm->movdqu->masm->mulsd->masm->addsd->masm->addsd->masm->recordExternalAddress->movdqu->masm->movdl->masm->subsd->masm->recordExternalAddress->leaq->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movq->masm->subsd->masm->movl->masm->andl->masm->addl->masm->new->AMD64Address->movdqu->masm->addsd->masm->pextrw->masm->shrl->masm->movl->masm->shrl->masm->subl->masm->shll->masm->movdl->masm->pshufd->masm->pshufd->masm->mulpd->masm->mulpd->masm->pshufd->masm->mulsd->masm->andl->masm->cmpl->masm->jcc->masm->mulsd->masm->paddd->masm->addpd->masm->mulsd->masm->pshufd->masm->mulpd->masm->addsd->masm->pshufd->masm->addl->masm->shll->masm->orl->masm->movdl->masm->mulsd->masm->mulsd->masm->addsd->masm->psllq->masm->addsd->masm->movdqu->masm->addsd->masm->mulsd->masm->pextrw->masm->andl->masm->jcc->masm->cmpl->masm->jcc->masm->bind->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->xorpd->masm->movl->masm->pinsrw->masm->addsd->masm->pextrw->masm->cmpl->masm->jcc->masm->xorpd->masm->movl->masm->pinsrw->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->movdl->masm->movdqu->masm->psrlq->masm->movdl->masm->orl->masm->jcc->masm->addsd->masm->movdqu->masm->jmp->masm->bind->masm->pextrw->masm->pextrw->masm->xorl->masm->testl->masm->jcc->masm->jmp->masm->bind->masm->pextrw->masm->andl->masm->pextrw->masm->xorpd->masm->subl->masm->xorl->masm->testl->masm->jcc->masm->jmp->masm->bind->masm->movl->masm->pinsrw->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->new->AMD64Address->movq->masm->bind->masm->addq
AMD64MathSinOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 math sin op emit code:::block0->new->Label->block1->new->Label->block2->new->Label->block3->new->Label->block4->new->Label->block5->new->Label->block6->new->Label->block7->new->Label->block8->new->Label->block9->new->Label->block10->new->Label->block11->new->Label->block12->new->Label->block13->new->Label->block14->new->Label->masm->push->masm->subq->masm->new->AMD64Address->movsd->masm->new->AMD64Address->movl->masm->recordExternalAddress->movq->masm->recordExternalAddress->movq->masm->andl->masm->subl->masm->cmpl->masm->jcc->masm->mulsd->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movq->masm->pand->masm->por->masm->addpd->masm->cvttsd2sil->masm->cvtsi2sdl->masm->recordExternalAddress->movdqu->masm->movq->masm->movdq->masm->recordExternalAddress->movdqu->masm->pshufd->masm->mulsd->if->masm->supports->masm->movddup->else->masm->movlhps->masm->andl->masm->shll->masm->recordExternalAddress->leaq->masm->addq->masm->mulpd->masm->recordExternalAddress->mulsd->masm->subsd->masm->new->AMD64Address->movq->masm->subsd->if->masm->supports->masm->movddup->else->masm->movdqu->masm->movlhps->masm->subsd->masm->pshufd->masm->new->AMD64Address->movdqu->masm->mulpd->masm->subpd->masm->mulsd->masm->subsd->masm->mulpd->masm->mulpd->masm->subsd->masm->recordExternalAddress->movdqu->masm->subsd->masm->new->AMD64Address->movq->masm->addsd->masm->subsd->masm->mulsd->masm->mulpd->masm->mulsd->masm->mulpd->masm->mulpd->masm->recordExternalAddress->addpd->masm->new->AMD64Address->mulsd->masm->recordExternalAddress->addpd->masm->mulpd->masm->movdqu->masm->new->AMD64Address->addsd->masm->mulpd->masm->movdqu->masm->addsd->masm->addpd->masm->new->AMD64Address->movq->masm->subsd->masm->subsd->masm->new->AMD64Address->addsd->masm->mulpd->masm->addsd->masm->addsd->masm->addsd->masm->addsd->masm->addsd->masm->unpckhpd->masm->movdqu->masm->addsd->masm->addsd->masm->jmp->masm->bind->masm->jcc->masm->shrl->masm->cmpl->masm->jcc->masm->recordExternalAddress->mulsd->masm->jmp->masm->bind->masm->recordExternalAddress->movq->masm->mulsd->masm->subsd->masm->recordExternalAddress->mulsd->masm->jmp->masm->bind->masm->pextrw->masm->andl->masm->cmpl->masm->jcc->masm->pextrw->masm->andl->masm->subl->masm->shrl->masm->andl->masm->recordExternalAddress->leaq->masm->addq->masm->movdq->masm->new->AMD64Address->movl->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->orl->masm->shrl->masm->movl->masm->imulq->masm->imulq->masm->imulq->masm->new->AMD64Address->movl->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->addq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->movl->masm->imulq->masm->imulq->masm->movl->masm->imulq->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->shlq->masm->orq->masm->imulq->masm->new->AMD64Address->movl->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->movq->masm->imulq->masm->imulq->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->movq->masm->imulq->masm->imulq->masm->shlq->masm->orq->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->imulq->masm->pextrw->masm->recordExternalAddress->leaq->masm->subq->masm->addl->masm->addl->masm->addl->masm->addl->masm->movl->masm->andl->masm->shrl->masm->andl->masm->subl->masm->subl->masm->addq->masm->movl->masm->addl->masm->cmpl->masm->jcc->masm->negl->masm->addl->masm->shll->masm->movl->masm->andl->masm->testl->masm->jcc->masm->shrl->masm->movl->masm->shlq->masm->orq->masm->bind->masm->bind->masm->cmpq->masm->jcc->masm->bind->masm->bsrq->masm->movl->masm->subl->masm->jcc->masm->shlq->masm->movq->masm->shlq->masm->addl->masm->negl->masm->addl->masm->shrq->masm->shrq->masm->orq->masm->orq->masm->bind->masm->cvtsi2sdq->masm->shrq->masm->cvtsi2sdq->masm->xorpd->masm->shll->masm->negl->masm->addl->masm->orl->masm->xorl->masm->pinsrw->masm->recordExternalAddress->movq->masm->recordExternalAddress->movq->masm->xorpd->masm->subl->masm->pinsrw->masm->mulsd->masm->shll->masm->sarl->masm->mulsd->masm->movdqu->masm->mulsd->masm->shrl->masm->addsd->masm->mulsd->masm->addl->masm->xorl->masm->mulsd->masm->movl->masm->addsd->masm->movdqu->masm->addsd->masm->subsd->masm->addsd->masm->bind->masm->recordExternalAddress->movq->masm->mulsd->masm->recordExternalAddress->movq->masm->recordExternalAddress->movq->masm->pand->masm->por->masm->addpd->masm->cvttsd2sil->masm->cvtsi2sdl->masm->recordExternalAddress->movq->masm->recordExternalAddress->movdqu->masm->mulsd->masm->unpcklpd->masm->shll->masm->addl->masm->movdqu->masm->addl->masm->andl->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->leaq->masm->shll->masm->addq->masm->mulpd->masm->subsd->masm->recordExternalAddress->mulsd->masm->subsd->masm->new->AMD64Address->movq->masm->unpcklpd->masm->movdqu->masm->subsd->masm->mulpd->masm->subpd->masm->mulsd->masm->subsd->masm->mulpd->masm->mulpd->masm->subsd->masm->new->AMD64Address->movdqu->masm->subsd->masm->new->AMD64Address->movq->masm->addsd->masm->subsd->masm->subsd->masm->recordExternalAddress->movdqu->masm->mulsd->masm->mulpd->masm->mulsd->masm->mulpd->masm->mulpd->masm->recordExternalAddress->addpd->masm->new->AMD64Address->mulsd->masm->recordExternalAddress->addpd->masm->mulpd->masm->movdqu->masm->new->AMD64Address->addsd->masm->mulpd->masm->movdqu->masm->addsd->masm->addpd->masm->new->AMD64Address->movq->masm->subsd->masm->subsd->masm->new->AMD64Address->addsd->masm->mulpd->masm->addsd->masm->addsd->masm->addsd->masm->addsd->masm->addsd->masm->unpckhpd->masm->movdqu->masm->addsd->masm->addsd->masm->jmp->masm->bind->masm->addl->masm->movq->masm->movq->masm->movl->masm->cmpq->masm->jcc->masm->addl->masm->movq->masm->movq->masm->cmpq->masm->jcc->masm->xorpd->masm->xorpd->masm->jmp->masm->bind->masm->jcc->masm->negl->masm->shrq->masm->movq->masm->shrq->masm->subl->masm->negl->masm->addl->masm->shlq->masm->orq->masm->jmp->masm->bind->masm->negl->masm->shlq->masm->orq->masm->shlq->masm->movq->masm->testl->masm->jcc->masm->shrl->masm->movl->masm->shrq->masm->jmp->masm->bind->masm->shrl->masm->movl->masm->shrl->masm->shlq->masm->orq->masm->shlq->masm->addl->masm->movl->masm->movl->masm->subq->masm->sbbq->masm->sbbq->masm->movq->masm->movq->masm->movq->masm->movl->masm->jmp->masm->bind->masm->shrl->masm->movq->masm->shrq->masm->movl->masm->movl->masm->subq->masm->sbbq->masm->sbbq->masm->movq->masm->movq->masm->movq->masm->movl->masm->shrq->masm->addl->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->recordExternalAddress->mulsd->masm->new->AMD64Address->movq->masm->bind->masm->addq->masm->pop
AMD64MathTanOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 math tan op emit code:::block0->new->Label->block1->new->Label->block2->new->Label->block3->new->Label->block4->new->Label->block5->new->Label->block6->new->Label->block7->new->Label->block8->new->Label->block9->new->Label->block10->new->Label->block11->new->Label->block12->new->Label->block13->new->Label->block14->new->Label->masm->push->masm->subq->masm->new->AMD64Address->movsd->masm->pextrw->masm->andl->masm->subl->masm->cmpl->masm->jcc->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movdqu->masm->unpcklpd->masm->recordExternalAddress->movdqu->masm->andpd->masm->recordExternalAddress->movdqu->masm->mulpd->masm->por->masm->addpd->masm->movdqu->masm->unpckhpd->masm->cvttsd2sil->masm->cvttpd2dq->masm->cvtdq2pd->masm->mulpd->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movq->masm->addq->masm->recordExternalAddress->movdqu->masm->mulpd->masm->andq->masm->mulsd->masm->movq->masm->mulpd->masm->shlq->masm->subpd->masm->recordExternalAddress->mulpd->masm->addq->masm->shlq->masm->addq->masm->addsd->masm->movdqu->masm->subpd->masm->recordExternalAddress->movq->masm->shlq->masm->recordExternalAddress->leaq->masm->recordExternalAddress->andpd->masm->movdqu->masm->addq->masm->subpd->masm->unpckhpd->masm->divsd->masm->subpd->masm->new->AMD64Address->movdqu->masm->subsd->masm->mulpd->masm->subpd->masm->new->AMD64Address->movdqu->masm->mulpd->masm->new->AMD64Address->movdqu->masm->mulpd->masm->addsd->masm->movdqu->masm->mulpd->masm->new->AMD64Address->addpd->masm->new->AMD64Address->addpd->masm->mulpd->masm->new->AMD64Address->addpd->masm->addpd->masm->new->AMD64Address->movdqu->masm->mulpd->masm->mulpd->masm->addpd->masm->new->AMD64Address->movdqu->masm->mulpd->masm->addpd->masm->movdqu->masm->mulpd->masm->mulsd->masm->new->AMD64Address->mulpd->masm->mulpd->masm->movdqu->masm->addpd->masm->movdqu->masm->mulsd->masm->unpckhpd->masm->addsd->masm->unpckhpd->masm->addsd->masm->subsd->masm->addsd->masm->movdqu->masm->new->AMD64Address->movq->masm->unpckhpd->masm->new->AMD64Address->addsd->masm->mulsd->masm->new->AMD64Address->addsd->masm->addsd->masm->addsd->masm->recordExternalAddress->movq->masm->mulsd->masm->new->AMD64Address->movq->masm->andpd->masm->mulsd->masm->new->AMD64Address->mulsd->masm->subsd->masm->new->AMD64Address->subsd->masm->subsd->masm->mulsd->masm->movdqu->masm->subsd->masm->addsd->masm->subsd->masm->addsd->masm->subsd->masm->addsd->masm->jmp->masm->bind->masm->jcc->masm->pextrw->masm->movl->masm->andl->masm->jcc->masm->andl->masm->cmpl->masm->jcc->masm->movdqu->masm->movdqu->masm->recordExternalAddress->movq->masm->mulsd->masm->mulsd->masm->mulsd->masm->recordExternalAddress->addsd->masm->mulsd->masm->recordExternalAddress->addsd->masm->mulsd->masm->recordExternalAddress->addsd->masm->mulsd->masm->recordExternalAddress->addsd->masm->mulsd->masm->addsd->masm->jmp->masm->bind->masm->recordExternalAddress->movq->masm->mulsd->masm->addsd->masm->recordExternalAddress->mulsd->masm->jmp->masm->bind->masm->movdqu->masm->mulsd->masm->jmp->masm->bind->masm->pextrw->masm->andl->masm->cmpl->masm->jcc->masm->pextrw->masm->andl->masm->subl->masm->shrl->masm->andl->masm->recordExternalAddress->leaq->masm->addq->masm->movdq->masm->new->AMD64Address->movl->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->orl->masm->shrl->masm->movl->masm->imulq->masm->imulq->masm->imulq->masm->new->AMD64Address->movl->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->addq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->movl->masm->imulq->masm->imulq->masm->movl->masm->imulq->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->shlq->masm->orq->masm->imulq->masm->new->AMD64Address->movl->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->movq->masm->imulq->masm->imulq->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->movq->masm->imulq->masm->imulq->masm->shlq->masm->orq->masm->new->AMD64Address->movl->masm->movl->masm->shrq->masm->addq->masm->movl->masm->shrq->masm->addq->masm->addq->masm->imulq->masm->pextrw->masm->recordExternalAddress->leaq->masm->subq->masm->addl->masm->addl->masm->addl->masm->addl->masm->movl->masm->andl->masm->shrl->masm->andl->masm->subl->masm->subl->masm->addq->masm->movl->masm->addl->masm->cmpl->masm->jcc->masm->negl->masm->addl->masm->shll->masm->movl->masm->andl->masm->testl->masm->jcc->masm->shrl->masm->movl->masm->shlq->masm->orq->masm->bind->masm->bind->masm->cmpq->masm->jcc->masm->bind->masm->bsrq->masm->movl->masm->subl->masm->jcc->masm->shlq->masm->movq->masm->shlq->masm->addl->masm->negl->masm->addl->masm->shrq->masm->shrq->masm->orq->masm->orq->masm->bind->masm->cvtsi2sdq->masm->shrq->masm->cvtsi2sdq->masm->xorpd->masm->shll->masm->negl->masm->addl->masm->orl->masm->xorl->masm->pinsrw->masm->recordExternalAddress->movq->masm->recordExternalAddress->movq->masm->xorpd->masm->subl->masm->pinsrw->masm->mulsd->masm->shll->masm->sarl->masm->mulsd->masm->movdqu->masm->mulsd->masm->shrl->masm->addsd->masm->mulsd->masm->addl->masm->xorl->masm->mulsd->masm->movl->masm->addsd->masm->movdqu->masm->addsd->masm->subsd->masm->addsd->masm->recordExternalAddress->movdqu->if->masm->supports->masm->movddup->else->masm->movlhps->masm->recordExternalAddress->movdqu->masm->andpd->masm->mulpd->if->masm->supports->masm->movddup->else->masm->movlhps->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movdqu->masm->por->masm->addpd->masm->movdqu->masm->unpckhpd->masm->cvttsd2sil->masm->cvttpd2dq->masm->cvtdq2pd->masm->mulpd->masm->recordExternalAddress->movdqu->masm->recordExternalAddress->movq->masm->shll->masm->addl->masm->recordExternalAddress->movdqu->masm->mulpd->masm->addl->masm->andl->masm->mulsd->masm->movl->masm->mulpd->masm->shll->masm->subpd->masm->recordExternalAddress->mulpd->masm->addl->masm->shll->masm->addl->masm->addsd->masm->movdqu->masm->subpd->masm->recordExternalAddress->movq->masm->shll->masm->recordExternalAddress->leaq->masm->recordExternalAddress->andpd->masm->movdqu->masm->addq->masm->subpd->masm->unpckhpd->masm->divsd->masm->subpd->masm->subsd->masm->subpd->masm->new->AMD64Address->movdqu->masm->addpd->masm->new->AMD64Address->movdqu->masm->mulpd->masm->new->AMD64Address->movdqu->masm->mulpd->masm->mulpd->masm->addsd->masm->movdqu->masm->mulpd->masm->new->AMD64Address->addpd->masm->new->AMD64Address->addpd->masm->mulpd->masm->new->AMD64Address->addpd->masm->addpd->masm->new->AMD64Address->movdqu->masm->mulpd->masm->mulpd->masm->addpd->masm->new->AMD64Address->movdqu->masm->mulpd->masm->addpd->masm->movdqu->masm->mulpd->masm->mulsd->masm->new->AMD64Address->mulpd->masm->mulpd->masm->movdqu->masm->addpd->masm->movdqu->masm->mulsd->masm->unpckhpd->masm->addsd->masm->unpckhpd->masm->addsd->masm->subsd->masm->addsd->masm->movdqu->masm->new->AMD64Address->movq->masm->unpckhpd->masm->new->AMD64Address->addsd->masm->mulsd->masm->new->AMD64Address->addsd->masm->addsd->masm->addsd->masm->recordExternalAddress->movq->masm->mulsd->masm->new->AMD64Address->movq->masm->andpd->masm->mulsd->masm->new->AMD64Address->mulsd->masm->subsd->masm->new->AMD64Address->subsd->masm->subsd->masm->mulsd->masm->movdqu->masm->subsd->masm->addsd->masm->subsd->masm->addsd->masm->subsd->masm->addsd->masm->jmp->masm->bind->masm->addl->masm->movq->masm->movq->masm->movl->masm->cmpq->masm->jcc->masm->addl->masm->movq->masm->movq->masm->cmpq->masm->jcc->masm->jmp->masm->bind->masm->jcc->masm->negl->masm->shrq->masm->movq->masm->shrq->masm->subl->masm->negl->masm->addl->masm->shlq->masm->orq->masm->jmp->masm->bind->masm->notl->masm->shlq->masm->orq->masm->shlq->masm->movq->masm->testl->masm->jcc->masm->shrl->masm->movl->masm->shrq->masm->jmp->masm->bind->masm->shrl->masm->movl->masm->shrl->masm->shlq->masm->orq->masm->shlq->masm->addl->masm->movl->masm->movl->masm->subq->masm->sbbq->masm->sbbq->masm->movq->masm->movq->masm->movq->masm->movl->masm->jmp->masm->bind->masm->shrl->masm->movq->masm->shrq->masm->movl->masm->movl->masm->subq->masm->sbbq->masm->sbbq->masm->movq->masm->movq->masm->movq->masm->movl->masm->shrq->masm->addl->masm->jmp->masm->bind->masm->new->AMD64Address->movq->masm->recordExternalAddress->mulsd->masm->new->AMD64Address->movq->masm->bind->masm->addq->masm->pop
AMD64Move.AbstractMoveOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: abstract move op emit code:::getResult->getInput->move
AMD64Move.MoveToRegOp#getInput()::: move to reg op get input:::return
AMD64Move.MoveToRegOp#getResult()::: move to reg op get result:::return
AMD64Move.MoveFromRegOp#getInput()::: move from reg op get input:::return
AMD64Move.MoveFromRegOp#getResult()::: move from reg op get result:::return
AMD64Move.MoveFromConstOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: move from const op emit code:::if->isRegister->asRegister->result->getPlatformKind->const2reg->else->const2stack
AMD64Move.MoveFromConstOp#getConstant()::: move from const op get constant:::return
AMD64Move.MoveFromConstOp#getResult()::: move from const op get result:::return
AMD64Move.AMD64StackMove#getInput()::: d64 stack move get input:::return
AMD64Move.AMD64StackMove#getResult()::: d64 stack move get result:::return
AMD64Move.AMD64StackMove#getScratchRegister()::: d64 stack move get scratch register:::return
AMD64Move.AMD64StackMove#getBackupSlot()::: d64 stack move get backup slot:::return
AMD64Move.AMD64StackMove#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 stack move emit code:::backupKind->backupSlot->getPlatformKind->if->backupKind->isXMM->reg2stack->getInput->getPlatformKind->getInput->stack2reg->getResult->getPlatformKind->getResult->reg2stack->stack2reg
AMD64Move.AMD64MultiStackMove#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 multi stack move emit code:::backupKind->backupSlot->getPlatformKind->if->backupKind->isXMM->scratch->backupSlot->getValueKind->asValue->move->for->i->scratch->backupSlot->getValueKind->asValue->move
AMD64Move.AMD64PushPopStackMove#getInput()::: d64 push pop stack move get input:::return
AMD64Move.AMD64PushPopStackMove#getResult()::: d64 push pop stack move get result:::return
AMD64Move.AMD64PushPopStackMove#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 push pop stack move emit code:::crb->asAddress->emit->crb->asAddress->emit
AMD64Move.LeaOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: lea op emit code:::if->masm->asRegister->address->toAddress->leaq->else->masm->asRegister->address->toAddress->lead
AMD64Move.LeaDataOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: lea data op emit code:::masm->asRegister->crb->recordDataReferenceInCode->leaq
AMD64Move.StackLeaOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: stack lea op emit code:::masm->asRegister->crb->asAddress->leaq
AMD64Move.MembarOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: membar op emit code:::masm->membar
AMD64Move.NullCheckOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: null check op emit code:::crb->masm->position->recordImplicitException->masm->address->toAddress->nullCheck
AMD64Move.NullCheckOp#getCheckedValue()::: null check op get checked value:::return
AMD64Move.NullCheckOp#getState()::: null check op get state:::return
AMD64Move.CompareAndSwapOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: compare and swap op emit code:::if->masm->lock->switch->masm->asRegister->address->toAddress->cmpxchgb->break->masm->asRegister->address->toAddress->cmpxchgw->break->masm->asRegister->address->toAddress->cmpxchgl->break->masm->asRegister->address->toAddress->cmpxchgq->break->throw->GraalError->shouldNotReachHere
AMD64Move.AtomicReadAndAddOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: atomic read and add op emit code:::move->if->masm->lock->switch->masm->address->toAddress->asRegister->xaddb->break->masm->address->toAddress->asRegister->xaddw->break->masm->address->toAddress->asRegister->xaddl->break->masm->address->toAddress->asRegister->xaddq->break->throw->GraalError->shouldNotReachHere
AMD64Move.AtomicReadAndWriteOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: atomic read and write op emit code:::move->switch->masm->asRegister->address->toAddress->xchgb->break->masm->asRegister->address->toAddress->xchgw->break->masm->asRegister->address->toAddress->xchgl->break->masm->asRegister->address->toAddress->xchgq->break->throw->GraalError->shouldNotReachHere
AMD64Move#move(CompilationResultBuilder, AMD64MacroAssembler, Value, Value)::: d64 move move:::result->getPlatformKind->move
AMD64Move#move(AMD64Kind, CompilationResultBuilder, AMD64MacroAssembler, Value, Value)::: d64 move move:::if->isRegister->if->isRegister->reg2reg->else->if->isStackSlot->asRegister->reg2stack->else->throw->GraalError->shouldNotReachHere->else->if->isStackSlot->if->isRegister->asRegister->stack2reg->else->throw->GraalError->shouldNotReachHere->else->if->isJavaConstant->if->isRegister->asRegister->asJavaConstant->const2reg->else->if->isStackSlot->asJavaConstant->const2stack->else->throw->GraalError->shouldNotReachHere->else->throw->GraalError->shouldNotReachHere
AMD64Move#reg2reg(AMD64Kind, AMD64MacroAssembler, Value, Value)::: d64 move reg2reg:::if->asRegister->asRegister->equals->return->switch->masm->asRegister->asRegister->movl->break->masm->asRegister->asRegister->movq->break->masm->asRegister->asRegister->movflt->break->masm->asRegister->asRegister->movdbl->break->throw->GraalError->shouldNotReachHere
AMD64Move#reg2stack(AMD64Kind, CompilationResultBuilder, AMD64MacroAssembler, Value, Register)::: d64 move reg2stack:::dest->crb->asAddress->switch->masm->movb->break->masm->movw->break->masm->movl->break->masm->movq->break->masm->movflt->break->masm->movsd->break->throw->GraalError->shouldNotReachHere
AMD64Move#stack2reg(AMD64Kind, CompilationResultBuilder, AMD64MacroAssembler, Register, Value)::: d64 move stack2reg:::src->crb->asAddress->switch->masm->movsbl->break->masm->movswl->break->masm->movl->break->masm->movq->break->masm->movflt->break->masm->movdbl->break->throw->GraalError->shouldNotReachHere
AMD64Move#const2reg(CompilationResultBuilder, AMD64MacroAssembler, Register, JavaConstant, AMD64Kind)::: d64 move const2reg:::switch->input->getJavaKind->getStackKind->masm->input->asInt->movl->break->if->input->asLong->input->asLong->masm->input->asLong->movslq->else->if->input->asLong->input->asLong->masm->input->asLong->movl->else->masm->input->asLong->movq->break->if->Float->input->asFloat->floatToRawIntBits->Float->floatToRawIntBits->masm->xorps->else->masm->crb->asFloatConstRef->movflt->break->if->Double->input->asDouble->doubleToRawLongBits->Double->doubleToRawLongBits->masm->xorpd->else->masm->crb->asDoubleConstRef->movdbl->break->if->input->isNull->if->crb->mustReplaceWithUncompressedNullRegister->masm->movq->else->masm->movslq->else->if->crb->recordInlineDataInCode->if->masm->movl->else->masm->movq->else->if->masm->crb->recordDataReferenceInCode->movl->else->masm->crb->recordDataReferenceInCode->movq->break->throw->GraalError->shouldNotReachHere
AMD64Move#canMoveConst2Stack(JavaConstant)::: d64 move can move const2 stack:::switch->input->getJavaKind->getStackKind->break->break->break->break->if->input->isNull->return->else->return->return->return
AMD64Move#const2stack(CompilationResultBuilder, AMD64MacroAssembler, Value, JavaConstant)::: d64 move const2stack:::dest->crb->asAddress->imm->switch->input->getJavaKind->getStackKind->input->asInt->break->input->asLong->break->input->asFloat->floatToRawIntBits->break->input->asDouble->doubleToRawLongBits->break->if->input->isNull->if->crb->mustReplaceWithUncompressedNullRegister->masm->movq->return->else->throw->GraalError->shouldNotReachHere->break->throw->GraalError->shouldNotReachHere->switch->result->getPlatformKind->emit->break->emit->break->masm->movl->break->masm->movlong->break->throw->GraalError->result->getPlatformKind->shouldNotReachHere
AMD64Move.PointerCompressionOp#hasBase(OptionValues, CompressEncoding)::: pointer compression op has base:::return->GeneratePIC->getValue->encoding->hasBase
AMD64Move.PointerCompressionOp#getInput()::: pointer compression op get input:::return
AMD64Move.PointerCompressionOp#getResult()::: pointer compression op get result:::return
AMD64Move.PointerCompressionOp#getResultRegister()::: pointer compression op get result register:::return->asRegister
AMD64Move.PointerCompressionOp#getBaseRegister(CompilationResultBuilder)::: pointer compression op get base register:::return->crb->getOptions->hasBase->asRegister
AMD64Move.PointerCompressionOp#getShift()::: pointer compression op get shift:::return->encoding->getShift
AMD64Move.PointerCompressionOp#move(LIRKind, CompilationResultBuilder, AMD64MacroAssembler)::: pointer compression op move:::AMD64Move->kind->getPlatformKind->move
AMD64Move.CompressPointerOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: compress pointer op emit code:::lirKindTool->getObjectKind->move->resReg->getResultRegister->baseReg->getBaseRegister->if->baseReg->equals->if->masm->testq->masm->cmovq->masm->subq->shift->getShift->if->masm->shrq
AMD64Move.UncompressPointerOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: uncompress pointer op emit code:::baseReg->getBaseRegister->if->baseReg->equals->getInput->inputReg->getInput->getRegister->if->inputReg->getResultRegister->equals->masm->getResultRegister->getShift->fromShift->new->AMD64Address->leaq->return->lirKindTool->getNarrowOopKind->move->getResultRegister->getShift->emitUncompressCode
AMD64Move.UncompressPointerOp#emitUncompressCode(AMD64MacroAssembler, Register, int, Register, boolean)::: uncompress pointer op emit uncompress code:::if->if->baseReg->equals->if->masm->fromShift->new->AMD64Address->leaq->else->masm->addq->else->if->masm->shlq->else->if->masm->shlq->if->baseReg->equals->if->masm->testq->done->new->Label->masm->jccb->masm->addq->masm->bind
AMD64Move.ZeroNullConversionOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: zero null conversion op emit code:::nullRegister->if->nullRegister->equals->asRegister->asRegister->emitConversion
AMD64Move.ZeroNullConversionOp#emitConversion(Register, Register, Register, AMD64MacroAssembler)::: zero null conversion op emit conversion:::
AMD64Move.ConvertNullToZeroOp#emitConversion(Register, Register, Register, AMD64MacroAssembler)::: convert null to zero op emit conversion:::if->inputRegister->equals->masm->subq->done->new->Label->masm->jccb->masm->addq->masm->bind->else->masm->subq->masm->cmpq->masm->cmovq
AMD64Move.ConvertZeroToNullOp#emitConversion(Register, Register, Register, AMD64MacroAssembler)::: convert zero to null op emit conversion:::if->inputRegister->equals->masm->movq->masm->testq->masm->cmovq
AMD64MulDivOp#getHighResult()::: d64 mul div op get high result:::return
AMD64MulDivOp#getLowResult()::: d64 mul div op get low result:::return
AMD64MulDivOp#getQuotient()::: d64 mul div op get quotient:::return
AMD64MulDivOp#getRemainder()::: d64 mul div op get remainder:::return
AMD64MulDivOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 mul div op emit code:::if->crb->masm->position->recordImplicitException->if->isRegister->opcode->asRegister->emit->else->opcode->crb->asAddress->emit
AMD64MulDivOp#verify()::: d64 mul div op verify:::if->else->if
AMD64PauseOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 pause op emit code:::asm->pause
AMD64PrefetchOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 prefetch op emit code:::switch->masm->address->toAddress->prefetchnta->break->masm->address->toAddress->prefetcht0->break->masm->address->toAddress->prefetcht2->break->masm->address->toAddress->prefetchw->break->throw->GraalError->shouldNotReachHere
AMD64ReadTimestampCounter#getHighResult()::: d64 read timestamp counter get high result:::return
AMD64ReadTimestampCounter#getLowResult()::: d64 read timestamp counter get low result:::return
AMD64ReadTimestampCounter#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 read timestamp counter emit code:::masm->rdtsc
AMD64RestoreRegistersOp#getSavedRegisters()::: d64 restore registers op get saved registers:::return
AMD64RestoreRegistersOp#restoreRegister(CompilationResultBuilder, AMD64MacroAssembler, Register, StackSlot)::: d64 restore registers op restore register:::AMD64Move->input->getPlatformKind->stack2reg
AMD64RestoreRegistersOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 restore registers op emit code:::savedRegisters->getSavedRegisters->for->i
AMD64SaveRegistersOp#saveRegister(CompilationResultBuilder, AMD64MacroAssembler, StackSlot, Register)::: d64 save registers op save register:::AMD64Move->result->getPlatformKind->reg2stack
AMD64SaveRegistersOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 save registers op emit code:::for->i
AMD64SaveRegistersOp#getSlots()::: d64 save registers op get slots:::return
AMD64SaveRegistersOp#supportsRemove()::: d64 save registers op supports remove:::return
AMD64SaveRegistersOp#remove(EconomicSet)::: d64 save registers op remove:::if->throw->new->UnsupportedOperationException->return->prune
AMD64SaveRegistersOp#prune(EconomicSet, Register[])::: d64 save registers op prune:::pruned->for->i->return
AMD64SaveRegistersOp#getMap(FrameMap)::: d64 save registers op get map:::total->for->i->keys->new->RegisterArr->values->new->intArr->if->mapIndex->for->i->return->new->RegisterSaveLayout
AMD64SaveRegistersOp#indexForStackSlot(FrameMap, StackSlot):::Computes the index of a stack slot relative to slot 0:::value->frameMap->offsetForStackSlot->frameMap->getTarget->return
AMD64ShiftOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 shift op emit code:::AMD64Move->move->opcode->asRegister->emit
AMD64ShiftOp#verify()::: d64 shift op verify:::
AMD64SignExtendOp#getHighResult()::: d64 sign extend op get high result:::return
AMD64SignExtendOp#getLowResult()::: d64 sign extend op get low result:::return
AMD64SignExtendOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 sign extend op emit code:::if->masm->cdql->else->masm->cdqq
AMD64SignExtendOp#verify()::: d64 sign extend op verify:::
AMD64StringLatin1InflateOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 string latin1 inflate op emit code:::src->asRegister->dst->asRegister->len->asRegister->tmp1->asRegister->tmp2->asRegister->byteArrayInflate
AMD64StringLatin1InflateOp#byteArrayInflate(AMD64MacroAssembler, Register, Register, Register, Register, Register):::Inflate a Latin1 string using a byte[] array representation into a UTF16 string using a char[] array representation.:::labelDone->new->Label->labelBelowThreshold->new->Label->if->masm->supports->masm->supports->masm->supports->masm->testl->masm->jcc->labelAvx512Tail->new->Label->masm->movl->masm->andl->masm->jccb->masm->andl->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->labelAvx512Loop->new->Label->masm->bind->masm->new->AMD64Address->evpmovzxbw->masm->new->AMD64Address->evmovdqu16->masm->addq->masm->jcc->masm->bind->masm->testl->masm->jcc->masm->kmovq->masm->movl->masm->shlxl->masm->notl->masm->kmovd->masm->new->AMD64Address->evpmovzxbw->masm->new->AMD64Address->evmovdqu16->masm->kmovq->masm->jmp->if->masm->supports->labelSSETail->new->Label->if->masm->supports->labelAvx2Tail->new->Label->masm->movl->masm->andl->masm->jccb->masm->andl->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->labelAvx2Loop->new->Label->masm->bind->masm->new->AMD64Address->vpmovzxbw->masm->new->AMD64Address->vmovdqu->masm->addq->masm->jcc->masm->bind->masm->bind->masm->movl->masm->andl->masm->jccb->masm->andl->masm->new->AMD64Address->pmovzxbw->masm->new->AMD64Address->movdqu->masm->addq->masm->addq->else->masm->movl->masm->andl->masm->jccb->masm->andl->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->labelSSECopy8Loop->new->Label->masm->bind->masm->new->AMD64Address->pmovzxbw->masm->new->AMD64Address->movdqu->masm->addq->masm->jcc->labelCopyChars->new->Label->masm->bind->masm->cmpl->masm->jccb->masm->new->AMD64Address->movdl->masm->pmovzxbw->masm->new->AMD64Address->movq->masm->subq->masm->addq->masm->addq->masm->bind->masm->testl->masm->jccb->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->labelCopyCharsLoop->new->Label->masm->bind->masm->new->AMD64Address->movzbl->masm->new->AMD64Address->movw->masm->incrementq->masm->jcc->masm->bind
AMD64StringUTF16CompressOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 string f16 compress op emit code:::res->asRegister->src->asRegister->dst->asRegister->len->asRegister->tmp1->asRegister->tmp2->asRegister->tmp3->asRegister->tmp4->asRegister->tmp5->asRegister->charArrayCompress
AMD64StringUTF16CompressOp#charArrayCompress(AMD64MacroAssembler, Register, Register, Register, Register, Register, Register, Register, Register, Register):::Compress a UTF16 string which de facto is a Latin1 string into a byte array representation (buffer).:::labelReturnLength->new->Label->labelReturnZero->new->Label->labelDone->new->Label->labelBelowThreshold->new->Label->masm->push->if->masm->supports->masm->supports->masm->supports->labelRestoreK1ReturnZero->new->Label->labelAvxPostAlignment->new->Label->masm->testl->masm->jcc->masm->movl->masm->evpbroadcastw->masm->kmovq->masm->testl->masm->jcc->masm->movl->masm->andl->masm->negl->masm->andl->masm->testl->masm->jcc->masm->movl->masm->shlxl->masm->notl->masm->kmovd->masm->new->AMD64Address->evmovdqu16->masm->evpcmpuw->masm->ktestd->masm->jcc->masm->new->AMD64Address->evpmovwb->masm->addq->masm->addq->masm->addq->masm->subl->masm->bind->labelAvx512LoopTail->new->Label->masm->movl->masm->andl->masm->jcc->masm->andl->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->labelAvx512Loop->new->Label->masm->bind->masm->new->AMD64Address->evmovdqu16->masm->evpcmpuw->masm->kortestd->masm->jcc->masm->new->AMD64Address->evpmovwb->masm->addq->masm->jcc->masm->bind->masm->kmovq->masm->testl->masm->jcc->masm->movl->masm->shlxl->masm->notl->masm->kmovd->masm->new->AMD64Address->evmovdqu16->masm->evpcmpuw->masm->ktestd->masm->jcc->masm->new->AMD64Address->evpmovwb->masm->kmovq->masm->jmp->masm->bind->masm->kmovq->masm->jmp->if->masm->supports->labelSSETail->new->Label->masm->bind->masm->movl->masm->movl->masm->andl->masm->jccb->masm->andl->masm->movdl->masm->pshufd->masm->pxor->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->lSSELoop->new->Label->masm->bind->masm->new->AMD64Address->movdqu->masm->new->AMD64Address->movdqu->masm->por->masm->por->masm->ptest->masm->jcc->masm->packuswb->masm->new->AMD64Address->movdqu->masm->addq->masm->jcc->labelCopyChars->new->Label->masm->bind->masm->movl->masm->andl->masm->jccb->masm->andl->masm->movdl->masm->pshufd->masm->pxor->masm->new->AMD64Address->movdqu->masm->ptest->masm->jccb->masm->packuswb->masm->new->AMD64Address->movq->masm->addq->masm->addq->masm->bind->masm->testl->masm->jccb->masm->new->AMD64Address->leaq->masm->new->AMD64Address->leaq->masm->negq->labelCopyCharsLoop->new->Label->masm->bind->masm->new->AMD64Address->movzwl->masm->testl->masm->jccb->masm->new->AMD64Address->movb->masm->incrementq->masm->jcc->masm->bind->masm->pop->masm->jmpb->masm->bind->masm->xorl->masm->addq->masm->bind
AMD64Ternary.ThreeOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: three op emit code:::AMD64Move->move->if->isRegister->opcode->asRegister->asRegister->asRegister->emit->else->opcode->asRegister->asRegister->crb->asAddress->emit
AMD64Unary.MOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: op emit code:::AMD64Move->move->opcode->asRegister->emit
AMD64Unary.RMOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: op emit code:::if->isRegister->opcode->asRegister->asRegister->emit->else->opcode->asRegister->crb->asAddress->emit
AMD64Unary.MROp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: op emit code:::if->isRegister->opcode->asRegister->asRegister->emit->else->opcode->crb->asAddress->asRegister->emit
AMD64Unary.MemoryOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: memory op emit code:::if->crb->masm->position->recordImplicitException->opcode->asRegister->input->toAddress->emit
AMD64Unary.MemoryOp#makeNullCheckFor(Value, LIRFrameState, int)::: memory op make null check for:::if->input->isValidImplicitNullCheckFor->return->return
AMD64VZeroUpper#initRegisterValues(Value[])::: d64 zero upper init register values:::skippedRegs->new->BitSet->numSkipped->if->foreach->if->isRegister->asRegister->getRegisterCategory->equals->skippedRegs->asRegister->set->regs->new->RegisterValueArr->for->i->j->return
AMD64VZeroUpper#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 zero upper emit code:::asm->vzeroupper
AMD64ZapRegistersOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 zap registers op emit code:::for->i
AMD64ZapRegistersOp#supportsRemove()::: d64 zap registers op supports remove:::return
AMD64ZapRegistersOp#remove(EconomicSet)::: d64 zap registers op remove:::return->prune
AMD64ZapRegistersOp#getMap(FrameMap)::: d64 zap registers op get map:::return->new->RegisterArr->new->intArr->new->RegisterSaveLayout
AMD64ZapStackOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 zap stack op emit code:::for->i
AMD64ZeroMemoryOp#emitCode(CompilationResultBuilder, AMD64MacroAssembler)::: d64 zero memory op emit code:::masm->pointer->toAddress->leaq->masm->xorq->masm->repStosb
